{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to the Malware Portion 👋\n",
    "\n",
    "In this section we’ll explore **adversarial machine learning for malware** using the **EMBER 2024 (feature v3)** dataset. Unlike older EMBER releases that depended on LIEF, v3 is **Colab-friendly**: features are extracted with a pure-Python pipeline and each Portable Executable (PE) is represented by a **fixed-length vector (2,568 features)**. These features capture signals like byte histograms, byte-entropy patterns, string statistics, and PE header metadata—compact summaries a classifier can learn from.\n",
    "\n",
    "We’ve already done the heavy lifting: a **multi-layer perceptron (MLP)** has been **trained on millions of Win32 samples** (many gigabytes). Your job now is to **use that trained model** to probe how sensitive it is to small changes in feature space and to try techniques similar to the image portion (e.g., FGSM/PGD-style moves, top-k edits).\n",
    "\n",
    "### What you’ll do here\n",
    "- **Load the trained model** and the stats needed to standardize features (mean/variance).\n",
    "- **Classify sample executables** and observe the model’s confidence.\n",
    "- **Run controlled feature-space experiments** (one-step and iterative, top-k changes) to see how targeted adjustments affect the score.\n",
    "- **Reflect on why changes work (or don’t)** by relating them back to feature families (byte hist, entropy, strings, headers).\n",
    "\n",
    "### How to get the most out of this\n",
    "- **Poke and prod.** Tweaking is encouraged—adjust `epsilon`, `k_top`, and which feature families you allow (`byte_hist`, `byte_entropy`, or `all`) and rerun cells to compare outcomes.\n",
    "- **Think in first-order terms.** Many methods rely on gradients (local sensitivity) to propose a direction; connect those proposals to the underlying features and constraints.\n",
    "- **Keep it safe and scientific.** We’ll stay in **feature space** for our manipulations—perfect for learning and discussion—without modifying real binaries.\n",
    "\n",
    "**Ready?** In the next cells, we’ll load the model and a test sample, then start experimenting.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environment setup (run once per session)\n",
    "\n",
    "This cell prepares your Colab runtime for the malware portion:\n",
    "\n",
    "> - Run this **once** when you open the notebook.\n",
    "> - If Colab restarts or you **Factory reset runtime**, run it again before continuing.\n",
    "\n",
    "After this completes, you’re ready to load the trained model and begin experiments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '/content'\n",
      "C:\\Users\\Student\\PycharmProjects\\imgMLforGithub\\malware_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\PycharmProjects\\imgMLforGithub\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\content\\Adversarial_ML25\\malware_classification\n",
      "✅ Cloned repo and changed directory to: /content/Adversarial_ML25/malware_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\PycharmProjects\\imgMLforGithub\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "REPO_URL=\"https://github.com/japheth45/Adversarial_ML25.git\"\n",
    "REPO_DIR=\"/content/Adversarial_ML25\"\n",
    "SUBDIR=\"$REPO_DIR/malware_classification\"\n",
    "REQ=\"$SUBDIR/requirements.txt\"\n",
    "\n",
    "rm -rf \"$REPO_DIR\"\n",
    "git clone \"$REPO_URL\" \"$REPO_DIR\" -q\n",
    "echo \"✅ Cloned into: $REPO_DIR\"\n",
    "cd \"$SUBDIR\"\n",
    "echo \"📂 CWD is now: $(pwd)\"\n",
    "\n",
    "python -m pip install --upgrade pip setuptools wheel\n",
    "echo \"📦 Installing requirements from: $REQ\"\n",
    "python -m pip install -r \"$REQ\"\n",
    "\n",
    "python - <<'PY'\n",
    "import sys, numpy, sklearn, torch\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "import thrember\n",
    "print(\"thrember:\", getattr(thrember, \"__version__\", \"import ok\"))\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paths & imports\n",
    "\n",
    "This cell defines the paths we’ll use throughout the malware section and makes the helper module importable:\n",
    "\n",
    "- `REPO_DIR` → the cloned workshop repo under `/content`\n",
    "- `SUBDIR` → the malware classification folder with helpers and models\n",
    "- `MODELS_DIR` → trained model + scaler stats\n",
    "- `DATA_DIR` / `PE_PATH` → where the sample PE lives\n",
    "\n",
    "Feel free to **change `PE_PATH` later** to try different files. Re-running this cell is fast and safe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "\n",
    "REPO_DIR = \"/content/Adversarial_ML25\"\n",
    "SUBDIR    = os.path.join(REPO_DIR, \"malware_classification\")\n",
    "MODELS_DIR = os.path.join(SUBDIR, \"models\")\n",
    "DATA_DIR   = os.path.join(SUBDIR, \"data\")\n",
    "PE_PATH    = os.path.join(DATA_DIR, \"toy_malware.exe\")\n",
    "\n",
    "# make helpers importable\n",
    "if SUBDIR not in sys.path:\n",
    "    sys.path.append(SUBDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline classification (sanity check)\n",
    "\n",
    "Let’s confirm everything is wired up by classifying our \"ransomware\" sample once with the **trained MLP**:\n",
    "\n",
    "- Asserts that the model artifacts and sample file exist.\n",
    "- Runs `classify_path(PE_PATH, MODELS_DIR)` to print the **logit**, **probability**, and **label**.\n",
    "- This gives us a starting point for the feature-space experiments that follow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from malware_helpers_v3 import classify_path\n",
    "\n",
    "assert os.path.exists(PE_PATH), f\"PE not found: {PE_PATH}\"\n",
    "assert os.path.isdir(MODELS_DIR), f\"Models dir not found: {MODELS_DIR}\"\n",
    "\n",
    "print(\"Classifying:\", PE_PATH)\n",
    "result = classify_path(PE_PATH, MODELS_DIR)\n",
    "print(json.dumps(result, indent=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single-step FGSM\n",
    "\n",
    "This cell applies a **Fast Gradient Sign Method (FGSM)** step to the EMBER v3 feature vector of our sample.\n",
    "\n",
    "1. **Extract & standardize** the 2,568-dim vector using the saved mean/variance.\n",
    "2. **Compute the gradient** of the BCE-with-logits loss with respect to the standardized input (for a benign target, we move in the direction that reduces the malicious score).\n",
    "3. **One-shot “sign” step:** take a single step of size `eps` in the gradient’s sign on all coordinates.\n",
    "4. **Visualize Δ:** the helper plots the per-feature change and saves\n",
    "   `original_scaled_vector_v3.npy` and `adversarial_scaled_vector_v3.npy` for reference.\n",
    "\n",
    "**Parameter to try**\n",
    "- `eps`: step size in standardized units (typical 0.01–0.10). Larger values push harder but can overshoot.\n",
    "\n",
    "**Reading the plot**\n",
    "- It’s a **line chart** over feature indices (0…2567). Adjacent features with opposite signs create vertical connections; long **white streaks** appear where adjacent features share the same sign (no crossing). Dense FGSM often looks like a band near ±`eps`.\n",
    "\n",
    "> This is a **single** step. For stronger effects, we’ll try an **iterative** variant later (recompute gradient → step again).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from malware_helpers_v3 import attack_and_plot_v3\n",
    "\n",
    "res = attack_and_plot_v3(\n",
    "    pe_path=PE_PATH,\n",
    "    models_dir=MODELS_DIR,\n",
    "    eps=0.05,                         # tweak epsilon to taste\n",
    "    plot_path=os.path.join(SUBDIR, \"v3_perturbation.png\"),\n",
    "    save_npy=True,\n",
    ")\n",
    "\n",
    "display(Image(filename=res[\"plot_path\"]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sparse FGSM (top-K features)\n",
    "\n",
    "In this step we run a **sparse** version of FGSM that updates only the **top-K most sensitive features** in **feature space**. Compared to the dense FGSM you just ran, this produces a **targeted** perturbation you can see clearly in the plot.\n",
    "\n",
    "**What the helper does:**\n",
    "1. **Extract & standardize** the EMBER v3 feature vector (2,568 dims).\n",
    "2. **Differentiate the loss** (BCE-with-logits) with respect to the standardized input to get a **saliency/gradient** for each feature.\n",
    "3. **Rank features by sensitivity** (magnitude of the gradient).\n",
    "   The `k` highest-magnitude coordinates are chosen as the **update set**.\n",
    "4. **One-shot “sign” step on K coords:** apply a step of size `ε` **only on those K features** (±ε depending on the target), leaving all others unchanged.\n",
    "5. **Plot** the per-feature change (Δ). With sparse FGSM you’ll see **K distinct needles** rather than a solid band.\n",
    "\n",
    "**Parameters to try**\n",
    "- `k`: how many features to change (e.g., 5, 50, 200). Smaller `k` → more interpretable; larger `k` → stronger but less surgical.\n",
    "- `eps` (`ε`): step size in standardized units (typical 0.01–0.10). Larger steps push harder but can overshoot.\n",
    "- `family`: restrict which feature families are eligible (`\"byte_hist\"`, `\"byte_entropy\"`, `\"generic\"`, `\"pe_only\"`, or `\"all\"`). This lets you ask *which block carries most leverage?*\n",
    "\n",
    "**Interpreting the output**\n",
    "- The printed summary shows the **baseline** and **after-step** probabilities, plus **how many features changed** (should equal `k` unless masked by `family`).\n",
    "- The plot shows Δ at each index; most entries will be zero, with **K spikes** at the selected coordinates.\n",
    "- If the score doesn’t move much, try **increasing `k` or `ε`**, or allow a broader `family`.\n",
    "\n",
    "> Tip: pair this sparse step with the dense FGSM plot you just saw to compare “spread the budget everywhere” vs “concentrate on a few coordinates.”\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from malware_helpers_v3 import attack_and_plot_topk_v3\n",
    "\n",
    "res = attack_and_plot_topk_v3(\n",
    "    pe_path=PE_PATH,                  # from your earlier cell\n",
    "    models_dir=MODELS_DIR,\n",
    "    k=200,\n",
    "    eps=0.05,\n",
    "    family=\"all\",                     # try 'byte_hist', 'byte_entropy', 'generic', or 'pe_only'\n",
    "    plot_path=f\"{SUBDIR}/v3_perturbation_topk.png\",\n",
    ")\n",
    "\n",
    "display(Image(filename=res[\"plot_path\"]))\n",
    "#res  # show the JSON-style summary too\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterative PGD (top-K, projected)\n",
    "\n",
    "Now we move from a single sign step to **Projected Gradient Descent (PGD)**—many small steps with a projection that keeps us inside a per-feature **L∞ budget**.\n",
    "\n",
    "**What this cell does**\n",
    "1. **Initialize** at the current standardized vector.\n",
    "2. For each iteration:\n",
    "   - **Compute gradient** toward the target.\n",
    "   - **Select top-K** most sensitive coordinates (by magnitude), filtered by `family` and any masks.\n",
    "   - **Step by `alpha`** on those K coordinates (sign of the gradient).\n",
    "   - **Project** back into the L∞ ball of radius `eps` (so no coordinate moves more than `eps` total).\n",
    "   - If `reselection=True`, re-pick the top-K each step (stronger but less stable); if `False`, keep the first set (easier to interpret).\n",
    "3. **Plot** the per-feature Δ after all steps.\n",
    "\n",
    "**Key parameters**\n",
    "- `k`: number of coordinates updated each step (e.g., 50–200). Smaller K → more surgical; larger K → stronger but more coupled.\n",
    "- `alpha`: step size per iteration (e.g., 0.005–0.02). If nothing moves, increase it; if you overshoot, reduce it.\n",
    "- `steps`: number of iterations (e.g., 10–40). More steps let the attack “track” curvature.\n",
    "- `eps`: maximum total change per coordinate (standardized units).\n",
    "- `family`: restrict updates to certain feature families (`\"byte_hist\"`, `\"byte_entropy\"`, `\"generic\"`, `\"pe_only\"`, or `\"all\"`).\n",
    "- `reselection`: `True` recomputes the K indices each step (usually stronger); `False` keeps a fixed set (more interpretable).\n",
    "\n",
    "**Reading the plot**\n",
    "- Expect **sparse points** at the indices updated across iterations. With reselection on, the pattern can shift as the optimizer adapts.\n",
    "- If the score barely changes, try **raising `alpha` or `steps`**, or allow a broader `family`. If it becomes unstable, reduce `alpha` or K.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from malware_helpers_v3 import attack_and_plot_pgd_topk_v3\n",
    "\n",
    "# Try 20 steps, alpha=0.01 first; bump steps or alpha if needed\n",
    "res = attack_and_plot_pgd_topk_v3(\n",
    "    pe_path=PE_PATH,                 # from your earlier cell\n",
    "    models_dir=MODELS_DIR,\n",
    "    k=200,\n",
    "    eps=0.05,\n",
    "    alpha=0.01,\n",
    "    steps=20,\n",
    "    family=\"all\",                    # or 'byte_hist', 'byte_entropy', 'generic', 'pe_only'\n",
    "    reselection=True,                # True often helps; try False for fixed set comparison\n",
    "    plot_path=f\"{SUBDIR}/v3_perturbation_pgd_topk.png\",\n",
    ")\n",
    "\n",
    "display(Image(filename=res[\"plot_path\"]))\n",
    "#res  # show the JSON-style summary too\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sparse L0 flip (find the smallest set of features that flips)\n",
    "\n",
    "This cell searches for the **smallest number of feature coordinates (k)** that must be changed—within the selected family—to push the model past a target margin (e.g., `prob ≤ 0.499` for benign).\n",
    "\n",
    "**How the search works**\n",
    "1. **Exponential search:** start at `k_min=1` and try k = 1,2,4,8,… until *some* k succeeds.\n",
    "2. **Binary search:** once a successful k is found, search downward to the **minimal k** that still meets the `margin`.\n",
    "3. **Inner optimizer (for each k):** with the current k-mask, run a short gradient-based routine for up to `inner_steps` to find a good perturbation; multiple `random_restarts` improve robustness.\n",
    "\n",
    "**Key controls**\n",
    "- `family`: restricts the pool of editable features (e.g., `\"byte_hist\"`). Use `\"all\"` to give the search more room.\n",
    "- `allowed_idx` / `exclude_idx`: optional fine-grained allow/deny lists.\n",
    "- `margin`: how far past the decision boundary we require (tighter → harder).\n",
    "- `inner_steps`, `inner_lr`: effort and step size of the inner optimization.\n",
    "- `random_restarts`, `patience`: help escape flat spots and tolerate brief non-improvement.\n",
    "- `max_outer_steps`: caps the exponential/binary search effort.\n",
    "\n",
    "**Output**\n",
    "- Prints the **baseline probability**, whether it **flipped**, the **minimal k** found, and **probability after**.\n",
    "- Returns a summary that includes the list of **changed indices** (inside the final k-mask) and saves the before/after standardized vectors (`original_scaled_vector_v3.npy`, `perturbed_scaled_vector_v3.npy`).\n",
    "\n",
    "> Tips: If the search stalls near the boundary, try increasing `inner_steps` (e.g., 400–800), `random_restarts`, or loosening `family` (e.g., from `\"byte_hist\"` to `\"all\"`).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from malware_helpers_v3 import run_sparse_flip_demo_v3\n",
    "\n",
    "res = run_sparse_flip_demo_v3(\n",
    "    pe_path=PE_PATH,\n",
    "    models_dir=MODELS_DIR,\n",
    "    k_min=1,\n",
    "    inner_steps=200,          # try 400–800 if you tighten margin\n",
    "    inner_lr=0.05,\n",
    "    family=\"byte_hist\",       # or 'all' to give the search more room\n",
    "    allowed_idx=None,         # e.g., [0,1,2] for precise allow-list\n",
    "    exclude_idx=None,         # e.g., [10,20,30] optional hard excludes\n",
    "    margin=0.499,             # tighter targets may need more effort\n",
    "    random_restarts=10,       # more restarts helps\n",
    "    patience=5,               # tolerate more no-improve steps\n",
    "    max_outer_steps=24,       # add a bit more room on the exp. search\n",
    "    save_npy=True,\n",
    ")\n",
    "#res  # show the JSON-style summary too\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overlay “closed loop” (exploratory)\n",
    "\n",
    "This cell runs an **overlay loop** that appends small chunks of bytes to a **copy** of the sample and measures how the model’s score changes after each attempt. It is intended as an **exploratory, educational demo** of how byte-level edits can influence feature vectors (e.g., byte histograms). It is **not** intended for real-world evasion.\n",
    "\n",
    "**What the loop does**\n",
    "1. **Baseline:** measure the model’s probability on the original file.\n",
    "2. **Propose a chunk:** pick a set of byte-histogram bins (controlled by `family` and `k_top`) and synthesize a small **overlay** chunk (`chunk_size_bytes`) targeting those bins.\n",
    "3. **Evaluate & accept:** apply the candidate to a **temporary copy**, re-score, and **accept** only if the score moves in the desired direction by at least `min_improve`.\n",
    "   - If the full chunk doesn’t help, the loop line-searches **smaller sub-chunks** (½, ¼, …) before counting it as a miss.\n",
    "4. **Stop conditions:** stop when the model crosses the **target threshold**, when **patience** is exhausted (no useful improvement), or when the **total appended bytes** hits `max_total_append`.\n",
    "5. **Optional shrink-to-fit:** after a successful step, try a smaller last chunk that still keeps the result.\n",
    "\n",
    "**Key controls**\n",
    "- `k_top`: how many bins to target per attempt (larger spreads the budget; smaller is more focused).\n",
    "- `chunk_size_bytes`: size of each candidate overlay (the loop will try smaller sub-chunks if needed).\n",
    "- `max_total_append`: global budget for all accepted overlays.\n",
    "- `target_threshold`: stopping goal for the model’s probability (start easier, tighten later).\n",
    "- `patience_steps` / `min_improve`: acceptance gate and early-stop behavior.\n",
    "- `family`: restricts which feature families can be targeted (e.g., `\"byte_hist\"`; try `\"all\"` or include `\"byte_entropy\"` for more leverage).\n",
    "\n",
    "**Output**\n",
    "- A `summary` showing the **baseline** and **final** probabilities, whether the target was met, **total bytes appended**, and a brief **history** of attempts.\n",
    "\n",
    "> **Note:** This step demonstrates *sensitivity* at the byte/feature level in a controlled setting. Results will vary by model and parameters, and the loop may stop early when improvements are too small or inconsistent.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from malware_helpers_v3 import OverlayConfig, run_overlay_closed_loop_v3\n",
    "\n",
    "cfg = OverlayConfig(\n",
    "    pe_in_path=f\"{SUBDIR}/data/toy_malware.exe\",\n",
    "    pe_out_path=f\"{SUBDIR}/data/demo_patch.exe\",\n",
    "    k_top=64,\n",
    "    chunk_size_bytes=1024,\n",
    "    max_total_append=1_048_576,\n",
    "    target_threshold=0.49,    # start easier, tighten after you see progress\n",
    "    patience_steps=10,\n",
    "    min_improve=1e-4,         # allow small but real gains to accumulate\n",
    "    shrink_to_fit=True,\n",
    "    seed=1337,\n",
    "    family=\"byte_hist\",       # try 'all' or include 'byte_entropy' for more leverage\n",
    "    allowed_idx=None,\n",
    "    exclude_idx=None,\n",
    ")\n",
    "summary = run_overlay_closed_loop_v3(cfg, models_dir=MODELS_DIR, device=\"cpu\")\n",
    "# summary\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterative, normalization-aware FGSM (feature space)\n",
    "\n",
    "Here we run a **multi-step** variant of FGSM that re-computes the gradient **each iteration** and only moves coordinates where an **addition is predicted to help** (normalization-aware selector):\n",
    "\n",
    "**What happens each step**\n",
    "1. **Gradient at the current point** (standardized features).\n",
    "2. **Normalization-aware selection:** pick up to `k_top` histogram coordinates whose “add-bytes” direction lowers the loss (i.e., safe directions; we skip bins that would hurt).\n",
    "3. **Nudge by `epsilon`** on those coordinates (optionally decaying `epsilon`), honoring an optional per-coordinate cap (`cap_linf`).\n",
    "4. **Accept** the step if the probability improves by at least `min_improve`; stop early if the **margin** is met (e.g., `p ≤ 0.49` for benign).\n",
    "\n",
    "**Parameters**\n",
    "- `steps`: maximum iterations (try 5–20).\n",
    "- `epsilon`: step size per iteration (e.g., 0.01–0.05); `epsilon_decay` can gently reduce it.\n",
    "- `k_top`: number of coordinates to change per step (smaller = more focused).\n",
    "- `family`: `\"all\"` or a subset like `\"byte_hist\"` to isolate where you want to act.\n",
    "- `cap_linf`: optional per-coordinate budget across all steps (standardized units).\n",
    "- `min_improve`, `patience`: simple monotonicity/early-stop controls.\n",
    "\n",
    "**Output**\n",
    "- Prints a small summary with the **before/after** probabilities, **how many steps** actually updated the vector, and the **number of coordinates** that received any change.\n",
    "- For deeper inspection, check `run[\"history\"]` to see per-step details (selected indices, step size, and acceptance).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import malware_helpers_v3 as mh\n",
    "reload(mh)\n",
    "\n",
    "model, mean, var, _ = mh.load_artifacts(MODELS_DIR)\n",
    "x0 = mh.extract_scaled_from_exe_v3(PE_PATH, mean=mean, var=var, device=\"cpu\")\n",
    "\n",
    "# Run 10 iterative steps, normalization-aware, feature-space only\n",
    "x_final, run = mh.fgsm_normaware_iter_v3(\n",
    "    x_scaled=x0, model=model, mean=mean, var=var,\n",
    "    steps=10, epsilon=0.03, k_top=50,\n",
    "    family=\"all\",               # or \"byte_hist\" to isolate\n",
    "    allowed_idx=None, exclude_idx=None,\n",
    "    target=None,                # flip current label\n",
    "    margin=0.49,                # stop once <= 0.49 (for benign)\n",
    "    epsilon_decay=1.0,          # keep step size constant; try 0.9 if you like\n",
    "    cap_linf=None,              # e.g., 0.2 to bound total per-dim movement\n",
    "    min_improve=0.0,\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "print({\n",
    "    \"before\": run[\"before_prob\"],\n",
    "    \"after\": run[\"after_prob\"],\n",
    "    \"steps_run\": run[\"steps_run\"],\n",
    "    \"changed_coords\": run[\"final_changed_coords\"],\n",
    "})\n",
    "# If you want, inspect run[\"history\"] to see per-step detail.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
