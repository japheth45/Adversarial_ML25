# Adversarial Machine Learning Workshop

Welcome! This repository contains all the code and notebooks for a hands-on workshop exploring the fascinating world of adversarial machine learning. Here you'll learn how AI models can be fooled with subtle, carefully crafted inputs.

---
## ðŸš€ Getting Started

The easiest way to participate is by using Google Colab, which provides a free, cloud-based environment. All you need is a web browser and a Google account. Once a notebook is open, run the cells from top to bottom.

---
## ðŸ“‚ Repository Structure

The workshop is divided into two main parts, each in its own folder:

### `image_classification/`
This folder contains the materials for the first, hands-on part of the workshop. We'll learn the fundamentals of adversarial attacks by crafting a perturbation to fool a state-of-the-art image classifier.

### `malware_classification/`
This folder contains a guided demonstration for the second part of the workshop. We'll explore how the same adversarial concepts can be applied in a cybersecurity context to help a malicious program evade an AI-powered malware detector.

### `legacy-code/`
This directory contains an archived version of a previous project, provided for historical/interest purposes only. It is not part of the main workshop. Please see the `README.md` file within that folder for more information.