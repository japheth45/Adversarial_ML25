#!/usr/bin/env python3
"""
new_classify.py â€” auto-detects checkpoint layout (Sequential net.* or named layers)
Usage:
  python new_classify.py <pe_file> --model artifacts/mlp_model.pth --scaler artifacts/scaler.joblib
"""

import argparse, time
from pathlib import Path

import ember
import numpy as np
import torch
import torch.nn as nn
from joblib import load
from sklearn.preprocessing import StandardScaler

# ---- Two model variants ------------------------------------------------------

class MalwareMLPSeq(nn.Module):
    """Sequential version with keys net.0, net.2, net.4"""
    def __init__(self, input_features=2381):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_features, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 1),
        )

    def forward(self, x):
        return self.net(x).squeeze(1)

class MalwareMLPNamed(nn.Module):
    """Named-layers version with keys layer1, layer2, output_layer"""
    def __init__(self, input_features=2381):
        super().__init__()
        self.layer1 = nn.Linear(input_features, 512)
        self.layer2 = nn.Linear(512, 256)
        self.output_layer = nn.Linear(256, 1)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.output_layer(x)
        return x.squeeze(1)

# ---- Globals (cached singletons) ---------------------------------------------

_MODEL = None
_SCALER = None
_EXTRACTOR = None

def get_scaler(path: Path) -> StandardScaler:
    global _SCALER
    if _SCALER is None:
        if not path.exists():
            raise FileNotFoundError(f"Scaler not found at {path}. Run run_once.py first.")
        _SCALER = load(path)
    return _SCALER

def _state_uses_named_layers(state_keys) -> bool:
    # Heuristic: if we see layer1.* or output_layer.* we assume named layout
    return any(k.startswith(("layer1", "layer2", "output_layer")) for k in state_keys)

def _remap_named_to_seq(state_dict):
    # Convert layer1/layer2/output_layer -> net.0/net.2/net.4
    key_map = {
        "layer1.weight": "net.0.weight",
        "layer1.bias":   "net.0.bias",
        "layer2.weight": "net.2.weight",
        "layer2.bias":   "net.2.bias",
        "output_layer.weight": "net.4.weight",
        "output_layer.bias":   "net.4.bias",
    }
    return { key_map.get(k, k): v for k, v in state_dict.items() }

def get_model(path: Path) -> nn.Module:
    global _MODEL
    if _MODEL is not None:
        return _MODEL

    if not path.exists():
        raise FileNotFoundError(f"Model weights not found at {path}.")

    state = torch.load(path, map_location="cpu")
    keys = list(state.keys())

    # If checkpoint uses named layers, prefer loading into the named model directly.
    if _state_uses_named_layers(keys):
        model = MalwareMLPNamed()
        try:
            model.load_state_dict(state, strict=True)
        except RuntimeError:
            # As a fallback, try remapping to sequential and load into seq model
            model = MalwareMLPSeq()
            state = _remap_named_to_seq(state)
            model.load_state_dict(state, strict=True)
    else:
        # Expect sequential keys net.0/net.2/net.4
        model = MalwareMLPSeq()
        try:
            model.load_state_dict(state, strict=True)
        except RuntimeError:
            # Fallback: maybe the file actually used named keys; try the other class
            model = MalwareMLPNamed()
            model.load_state_dict(state, strict=True)

    model.eval()
    _MODEL = model
    return _MODEL

def get_extractor(feature_version: int = 2):
    global _EXTRACTOR
    if _EXTRACTOR is None:
        _EXTRACTOR = ember.PEFeatureExtractor(feature_version=feature_version)
    return _EXTRACTOR

# ---- Classification -----------------------------------------------------------

def classify_file(pe_path: Path, model_path: Path, scaler_path: Path, feature_version: int = 2):
    t0 = time.time()

    scaler = get_scaler(scaler_path)
    model  = get_model(model_path)
    extractor = get_extractor(feature_version=feature_version)

    t_read0 = time.time()
    with open(pe_path, "rb") as f:
        bytez = f.read()
    t_read1 = time.time()

    feats = extractor.feature_vector(bytez)
    feats = np.nan_to_num(feats, copy=False)
    t_feat = time.time()

    feats_scaled = scaler.transform(feats.reshape(1, -1))
    t_scale = time.time()

    x = torch.from_numpy(feats_scaled.astype(np.float32))
    with torch.no_grad():
        logit = model(x)
        prob  = torch.sigmoid(logit)[0].item()
    t_fwd = time.time()

    label = "MALICIOUS" if prob >= 0.5 else "BENIGN"
    score = prob if prob >= 0.5 else 1.0 - prob

    print("\n--- Classification Result ---")
    print(f"File: {pe_path}")
    print(f"Prediction: {label}")
    print(f"Raw malicious probability: {prob:.4f}")
    # print("\n--- Timing (seconds) ---")
    # print(f"Total:     {t_fwd - t0:7.3f}")
    # print(f"  Read:    {t_read1 - t_read0:7.3f}")
    # print(f"  Extract: {t_feat  - t_read1:7.3f}")
    # print(f"  Scale:   {t_scale - t_feat:7.3f}")
    # print(f"  Forward: {t_fwd   - t_scale:7.3f}")

def main():
    p = argparse.ArgumentParser(description="Fast single-file EMBER classifier (auto-detects checkpoint layout).")
    p.add_argument("file", help="Path to PE file to classify.")
    p.add_argument("--model",  default="artifacts/mlp_model.pth",   help="Path to model .pth")
    p.add_argument("--scaler", default="artifacts/scaler.joblib",   help="Path to StandardScaler (joblib)")
    p.add_argument("--feature_version", type=int, default=2, help="EMBER feature version (2 = 2018)")
    args = p.parse_args()

    pe_path = Path(args.file)
    if not pe_path.exists():
        raise FileNotFoundError(f"Input file not found: {pe_path}")

    classify_file(pe_path, Path(args.model), Path(args.scaler), feature_version=args.feature_version)

if __name__ == "__main__":
    main()
