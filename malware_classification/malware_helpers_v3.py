# malware_helpers_v3.py
import os, json, numpy as np, torch, torch.nn as nn
import random

class MalwareMLP(nn.Module):
    def __init__(self, d_in, hidden=(512, 256), p=0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, hidden[0]), nn.ReLU(), nn.Dropout(p),
            nn.Linear(hidden[0], hidden[1]), nn.ReLU(), nn.Dropout(p),
            nn.Linear(hidden[1], 1)  # logits
        )
    def forward(self, x): return self.net(x)

def load_artifacts(models_dir):
    with open(os.path.join(models_dir, "mlp_config.json"), "r") as f:
        cfg = json.load(f)
    with open(os.path.join(models_dir, "scaler_stats.json"), "r") as f:
        stats = json.load(f)
    mean = np.asarray(stats["mean"], dtype=np.float32)
    var  = np.asarray(stats["var"],  dtype=np.float32)
    model = MalwareMLP(d_in=cfg["input_dim"], hidden=tuple(cfg["hidden"]), p=cfg.get("dropout", 0.2))
    sd = torch.load(os.path.join(models_dir, "mlp_state_dict.pth"), map_location="cpu")
    model.load_state_dict(sd, strict=True)
    model.eval()
    return model, mean, var, cfg

def extract_v3_features(pe_path):
    """
    Get a 2568-dim v3 PE vector via thrember. Handles common API shapes.
    """
    import thrember
    # Preferred: class-based extractor
    PEFE = getattr(thrember, "PEFeatureExtractor", None)
    if PEFE is not None:
        extractor = PEFE()
        if hasattr(extractor, "feature_vector_from_path"):
            vec = extractor.feature_vector_from_path(pe_path)
            return np.asarray(vec, dtype=np.float32)
        if hasattr(extractor, "feature_vector"):
            with open(pe_path, "rb") as f:
                raw = f.read()
            vec = extractor.feature_vector(raw)
            return np.asarray(vec, dtype=np.float32)
    # Fallback: module-level helpers (if present)
    for fn_name in ("vectorize_file", "feature_vector_from_path"):
        fn = getattr(thrember, fn_name, None)
        if callable(fn):
            vec = fn(pe_path)
            return np.asarray(vec, dtype=np.float32)
    raise RuntimeError("Could not locate a thrember PE v3 single-file extractor.")

def standardize(vec, mean, var):
    vec = vec.astype(np.float32)
    return (vec - mean) / np.sqrt(var)

def classify_path(pe_path, models_dir):
    model, mean, var, cfg = load_artifacts(models_dir)
    vec = extract_v3_features(pe_path)
    D = cfg["input_dim"]
    if vec.shape[0] != D:
        raise ValueError(f"Feature dim mismatch: got {vec.shape[0]}, expected {D}")
    x = standardize(vec, mean, var)[None, :]
    with torch.no_grad():
        logit = model(torch.from_numpy(x)).item()
        prob = torch.sigmoid(torch.tensor(logit)).item()
    label = "MALICIOUS" if prob >= cfg.get("threshold", 0.5) else "BENIGN"
    return {"path": pe_path, "logit": logit, "prob": prob, "label": label}

# --- v3 adversarial FGSM (L-infinity) + plotting helpers ---

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import os

# Small epsilon default; feel free to adjust in the notebook
_DEFAULT_EPS = 0.05

def _predict_logit_prob_from_stdvec(model, x_std_np):
    """x_std_np: shape (D,) standardized np.float32"""
    x = torch.from_numpy(x_std_np[None, :])
    with torch.no_grad():
        logit = model(x).item()
        prob = torch.sigmoid(torch.tensor(logit)).item()
    return logit, prob

def fgsm_linf_attack_v3(
    model,
    x_std_np: np.ndarray,
    target_prob: float = 0.0,
    eps: float = _DEFAULT_EPS,
    return_sign: bool = False,
):
    """
    One-step FGSM on standardized features (L∞).
    - x_std_np: (D,) np.float32, standardized features
    - target_prob: 0.0 (benign) or 1.0 (malicious)
    - eps: L∞ budget (applied in standardized space)
    Returns:
      x_adv_std: (D,) np.float32 after one FGSM step
      (optionally) grad_sign: (D,) int8 in {-1,0,+1}
    """
    # build a 1xD tensor requiring grad
    x = torch.from_numpy(x_std_np[None, :]).clone().detach().requires_grad_(True)

    # label as a probability target; BCEWithLogitsLoss expects logits vs prob target
    target = torch.tensor([[target_prob]], dtype=torch.float32)
    criterion = nn.BCEWithLogitsLoss()

    logits = model(x)
    loss = criterion(logits, target)
    model.zero_grad()
    loss.backward()

    # sign of gradient (zeros remain zero)
    g = x.grad.detach().squeeze(0).cpu().numpy().astype(np.float32)   # (D,)
    sign = np.sign(g).astype(np.int8)                                  # {-1,0,+1}

    # FGSM step *toward* the target: subtract sign when target=0 (benign),
    # add sign when target=1 (malicious). Using a single formula:
    direction = -1.0 if target_prob <= 0.5 else +1.0
    step = (direction * eps * sign).astype(np.float32)

    x_adv = (x.detach().squeeze(0).cpu().numpy().astype(np.float32) + step)

    if return_sign:
        return x_adv, sign
    return x_adv


def feature_block_v3(idx):
    """
    Coarse feature family labels (v3).
    0–255: byte histogram
    256–511: byte-entropy histogram
    512–695: generic strings/stats
    696–2567: PE-only (headers/sections/imports/exports/ddirs/authenticode/warnings)
    """
    if   0 <= idx <= 255:   return "Byte histogram"
    elif 256 <= idx <= 511: return "Byte-entropy histogram"
    elif 512 <= idx <= 695: return "Generic strings & stats"
    else:                   return "PE-only features"

def attack_and_plot_v3(
    pe_path,
    models_dir,
    eps=_DEFAULT_EPS,
    plot_path="v3_perturbation.png",
    save_npy=True,
    tol=1e-7,
):
    """
    End-to-end FGSM L∞ demo:
      - extract v3 features -> standardize
      - score baseline
      - FGSM toward benign (target_prob=0.0)
      - re-score
      - save original/adversarial standardized vectors
      - plot per-feature deltas
      - print precise change counts (nonzero-grad, exactly ±eps, etc.)
    Returns a dict with metrics and paths.
    """
    top_k = 5

    # 1) load & featurize
    model, mean, var, cfg = load_artifacts(models_dir)
    vec = extract_v3_features(pe_path)  # (D,)
    D = cfg["input_dim"]
    if vec.shape[0] != D:
        raise ValueError(f"Feature dim mismatch: got {vec.shape[0]}, expected {D}")
    x0_std = standardize(vec, mean, var).astype(np.float32)

    # 2) baseline scores
    logit0, prob0 = _predict_logit_prob_from_stdvec(model, x0_std)

    # 3) FGSM toward benign and capture grad sign
    x_adv_std, grad_sign = fgsm_linf_attack_v3(
        model, x0_std, target_prob=0.0, eps=eps, return_sign=True
    )

    # 4) post-attack scores
    logit1, prob1 = _predict_logit_prob_from_stdvec(model, x_adv_std)

    # 5) vectors + diffs
    if save_npy:
        np.save("original_scaled_vector_v3.npy", x0_std)
        np.save("adversarial_scaled_vector_v3.npy", x_adv_std)

    delta = (x_adv_std - x0_std).astype(np.float32)
    abs_delta = np.abs(delta)

    # precise counts
    nonzero_grad = int(np.count_nonzero(grad_sign))
    zero_grad = int(D - nonzero_grad)
    changed = int(np.count_nonzero(abs_delta > tol))

    # how many are "exactly" ±eps (within tiny tolerance to allow float noise)
    eps_hits = int(np.count_nonzero(np.isclose(abs_delta, eps, atol=1e-6)))

    # 6) plot per-feature Δ
    plt.figure(figsize=(12, 5))
    plt.plot(delta, linewidth=0.8)
    plt.title(f"FGSM L∞ perturbation per feature (ε={eps})")
    plt.xlabel("Feature index (0…2567)")
    plt.ylabel("Δ (standardized feature space)")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(plot_path)
    plt.close()

    # 7) top-k movers (by |Δ|)
    sorted_idx = np.argsort(abs_delta)[::-1]
    top = []
    for i in range(min(top_k, D)):
        j = int(sorted_idx[i])
        top.append({
            "index": j,
            "delta": float(delta[j]),
            "family": feature_block_v3(j),
        })

    # 8) summary print
    print(f"\nFile: {pe_path}")
    print(f"Baseline: logit={logit0:.4f}, prob={prob0:.4f}")
    print(f"After FGSM (ε={eps}): logit={logit1:.4f}, prob={prob1:.4f}")
    print(f"Features with nonzero grad: {nonzero_grad:,}  (zero-grad: {zero_grad:,})")
    print(f"Features changed (|Δ| > {tol:g}): {changed:,}")
    print(f"Features changed by ≈ε (|Δ| ≈ {eps}): {eps_hits:,}")
    for i, t in enumerate(top, 1):
        print(f"  {i}. idx={t['index']:4d}  Δ={t['delta']:+.4f}  fam={t['family']}")
    print(f"Plot saved: {plot_path}\n")

    # 9) return stats
    summary = {
        "file": pe_path,
        "epsilon": float(eps),
        "before": {"logit": float(logit0), "prob": float(prob0)},
        "after":  {"logit": float(logit1), "prob": float(prob1)},
        "nonzero_grad": nonzero_grad,
        "zero_grad": zero_grad,
        "changed_features": changed,
        "changed_abs_eps": eps_hits,
        "plot_path": plot_path,
        "top_changes": top,
    }
    return summary

# -------- Sparse FGSM (top-k) helpers for v3 --------

# ---- Mask builders (combine families + explicit allow/exclude) ----

def family_mask_v3(family: str | None, D: int = 2568) -> np.ndarray:
    """Return boolean mask for a coarse family, or all True if family is None/'all'."""
    m = np.zeros(D, dtype=bool)
    if family in (None, "all"):
        m[:] = True
    elif family == "byte_hist":
        m[0:256] = True
    elif family == "byte_entropy":
        m[256:512] = True
    elif family == "generic":
        m[512:696] = True
    elif family == "pe_only":
        m[696:D] = True
    else:
        raise ValueError(f"Unknown family '{family}'")
    return m

# Back-compat alias
def _family_mask_v3(family: str | None, D: int = 2568):
    return family_mask_v3(family, D)


def build_editable_mask(
    D: int,
    family: str | None = None,
    allowed_idx: list[int] | np.ndarray | None = None,
    exclude_idx: list[int] | np.ndarray | None = None,
) -> np.ndarray:
    """
    Combine a coarse family mask with explicit allow / exclude lists.
    If allowed_idx is given, we intersect with the family mask.
    exclude_idx always wins (they're removed).
    """
    base = family_mask_v3(family, D=D)
    if allowed_idx is not None:
        a = np.zeros(D, dtype=bool)
        ai = np.asarray(allowed_idx, dtype=int)
        ai = ai[(ai >= 0) & (ai < D)]
        a[ai] = True
        base &= a
    if exclude_idx is not None:
        ei = np.asarray(exclude_idx, dtype=int)
        ei = ei[(ei >= 0) & (ei < D)]
        base[ei] = False
    return base

def fgsm_topk_attack_v3(
    model,
    x_std_np: np.ndarray,
    k: int = 200,
    target_prob: float = 0.0,
    eps: float = _DEFAULT_EPS,
    mask: np.ndarray | None = None,
):
    """
    Top-k sparse FGSM on standardized features.
    - x_std_np: (D,) np.float32 standardized vector
    - k: number of features to modify (chosen by |grad|)
    - target_prob: 0.0 (benign) or 1.0 (malicious)
    - eps: step size (L∞ budget)
    - mask: optional boolean array (D,) that restricts where we can change
            (e.g. _family_mask_v3('byte_hist')); if None, allow all.
    Returns:
      x_adv_std: (D,) after attack
      idx_sel:  indices changed (np.int64, length <= k)
      grad:     raw gradient (np.float32, (D,))
    """
    D = x_std_np.shape[0]
    x = torch.from_numpy(x_std_np[None, :]).clone().detach().requires_grad_(True)
    target = torch.tensor([[target_prob]], dtype=torch.float32)
    criterion = nn.BCEWithLogitsLoss()

    logits = model(x)
    loss = criterion(logits, target)
    model.zero_grad(); loss.backward()

    g = x.grad.detach().squeeze(0).cpu().numpy().astype(np.float32)  # raw grads
    g_abs = np.abs(g)

    # restrict by mask if provided
    if mask is not None:
        if mask.shape[0] != D:
            raise ValueError("mask size must match feature dim")
        g_abs = np.where(mask, g_abs, 0.0)

    # pick top-k sensitive indices
    if k >= D:
        idx_sel = np.argsort(g_abs)[::-1]
    else:
        # argpartition is faster for large D
        idx_sel = np.argpartition(g_abs, -k)[-k:]
        idx_sel = idx_sel[np.argsort(g_abs[idx_sel])[::-1]]

    # build sparse step
    step = np.zeros(D, dtype=np.float32)
    sgn = np.sign(g).astype(np.float32)
    direction = -1.0 if target_prob <= 0.5 else +1.0
    step[idx_sel] = direction * eps * sgn[idx_sel]

    x_adv = (x.detach().squeeze(0).cpu().numpy().astype(np.float32) + step)
    return x_adv, idx_sel.astype(np.int64), g


def attack_and_plot_topk_v3(
    pe_path,
    models_dir,
    k: int = 200,
    eps: float = _DEFAULT_EPS,
    family: str | None = None,            # e.g., 'all', 'byte_hist', 'byte_entropy', 'generic', 'pe_only'
    plot_path: str = "v3_perturbation_topk.png",
    save_npy: bool = True,
):
    """
    End-to-end demo:
      - extract v3 -> standardize
      - baseline score
      - sparse FGSM on top-k features (optionally restricted to a family)
      - after score, save vectors, plot sparse deltas, and summarize.
    """
    model, mean, var, cfg = load_artifacts(models_dir)
    vec = extract_v3_features(pe_path)
    D = cfg["input_dim"]
    if vec.shape[0] != D:
        raise ValueError(f"Feature dim mismatch: got {vec.shape[0]}, expected {D}")
    x0 = standardize(vec, mean, var).astype(np.float32)

    logit0, prob0 = _predict_logit_prob_from_stdvec(model, x0)

    m = _family_mask_v3(family or "all", D=D)
    xk, idx_sel, grad = fgsm_topk_attack_v3(model, x0, k=k, target_prob=0.0, eps=eps, mask=m)
    logit1, prob1 = _predict_logit_prob_from_stdvec(model, xk)

    if save_npy:
        np.save("original_scaled_vector_v3.npy", x0)
        np.save("adversarial_scaled_vector_v3.npy", xk)

    delta = (xk - x0).astype(np.float32)
    abs_delta = np.abs(delta)
    eps_hits = int(np.count_nonzero(np.isclose(abs_delta[idx_sel], eps, atol=1e-6)))
    changed = int((abs_delta > 0).sum())
    # rank changed indices by |grad| (more informative than |Δ| when Δ=ε on all selected)
    rk = idx_sel[np.argsort(np.abs(grad[idx_sel]))[::-1]]

    # Plot (sparse): scatter is clearer than a connected line
    plt.figure(figsize=(12, 4))
    plt.scatter(idx_sel, delta[idx_sel], s=8)
    plt.title(f"Top-{k} FGSM L∞ per-feature Δ (ε={eps}, family={family or 'all'})")
    plt.xlabel("Feature index (0…2567)")
    plt.ylabel("Δ (standardized)")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(plot_path); plt.close()

    # Top-5 report
    top = []
    for j in rk[:5]:
        top.append({
            "index": int(j),
            "delta": float(delta[j]),
            "grad_abs": float(abs(grad[j])),
            "family": feature_block_v3(int(j)),
        })

    print(f"\nFile: {pe_path}")
    print(f"Baseline: logit={logit0:.4f}, prob={prob0:.4f}")
    print(f"After top-{k} FGSM (ε={eps}, family={family or 'all'}): logit={logit1:.4f}, prob={prob1:.4f}")
    print(f"Changed features (nonzero Δ): {changed:,} (selected top-k={len(idx_sel)})")
    print(f"Of selected, |Δ| ≈ ε: {eps_hits}/{len(idx_sel)}")
    for i, t in enumerate(top, 1):
        print(f"  {i}. idx={t['index']:4d}  Δ={t['delta']:+.4f}  |grad|={t['grad_abs']:.4e}  fam={t['family']}")
    print(f"Plot saved: {plot_path}\n")

    return {
        "file": pe_path,
        "epsilon": float(eps),
        "k": int(k),
        "family": family or "all",
        "before": {"logit": float(logit0), "prob": float(prob0)},
        "after":  {"logit": float(logit1), "prob": float(prob1)},
        "selected_indices": idx_sel.tolist(),
        "changed_features": changed,
        "eps_hits_selected": int(eps_hits),
        "plot_path": plot_path,
        "top_changes": top,
    }

# -------- Masked PGD (top-k, L∞) for v3 --------

def pgd_topk_attack_v3(
    model,
    x_std_np: np.ndarray,
    k: int = 200,
    target_prob: float = 0.0,
    eps: float = _DEFAULT_EPS,
    alpha: float = 0.01,
    steps: int = 20,
    mask: np.ndarray | None = None,   # boolean (D,), e.g., _family_mask_v3('byte_hist')
    random_start: bool = True,
    reselection: bool = False,        # if True, re-pick top-k by |grad| each step (within mask)
):
    """
    Masked PGD in standardized space with a sparse L0 (top-k) + L∞ budget.
    - x_std_np: (D,) np.float32, baseline standardized vector
    - k: number of features that can change
    - target_prob: 0.0 (benign) or 1.0 (malicious)
    - eps: max |delta_i| per selected coordinate (L∞)
    - alpha: step size per iteration
    - steps: number of PGD iterations
    - mask: restrict pool of coordinates eligible for selection (None = all)
    - random_start: start from a random point inside the ε-ball on selected coords
    - reselection: if True, recompute top-k (by |grad|) each step; else keep initial set
    Returns:
      x_adv_std (D,), idx_sel (np.int64, ≤k), grad_last (D,)
    """
    D = x_std_np.shape[0]
    direction = -1.0 if target_prob <= 0.5 else +1.0

    # helper: gradient at current x
    def _grad_at(x_vec):
        x = torch.from_numpy(x_vec[None, :]).clone().detach().requires_grad_(True)
        target = torch.tensor([[target_prob]], dtype=torch.float32)
        logits = model(x)
        loss = nn.BCEWithLogitsLoss()(logits, target)
        model.zero_grad(); loss.backward()
        g = x.grad.detach().squeeze(0).cpu().numpy().astype(np.float32)
        return g

    # initial selection by |grad| (within mask)
    g0 = _grad_at(x_std_np)
    g_abs = np.abs(g0)
    if mask is not None:
        if mask.shape[0] != D:
            raise ValueError("mask size must match feature dim")
        g_abs = np.where(mask, g_abs, 0.0)

    if k >= D:
        idx_sel = np.argsort(g_abs)[::-1]
    else:
        idx_sel = np.argpartition(g_abs, -k)[-k:]
        idx_sel = idx_sel[np.argsort(g_abs[idx_sel])[::-1]]
    idx_sel = idx_sel.astype(np.int64)

    # init delta
    delta = np.zeros(D, dtype=np.float32)
    if random_start:
        delta[idx_sel] = np.random.uniform(low=-eps, high=eps, size=idx_sel.size).astype(np.float32)

    # project to feasible set (sparse L0 on idx_sel, L∞ bound)
    def _project(delta_vec, active_idx):
        out = np.zeros_like(delta_vec)
        out[active_idx] = np.clip(delta_vec[active_idx], -eps, eps)
        return out

    delta = _project(delta, idx_sel)

    # PGD loop
    x0 = x_std_np.astype(np.float32)
    grad_last = g0
    for _ in range(steps):
        x_cur = x0 + delta
        g = _grad_at(x_cur)
        grad_last = g

        # optionally reselect top-k each step (within mask)
        if reselection:
            g_abs = np.abs(g)
            if mask is not None:
                g_abs = np.where(mask, g_abs, 0.0)
            if k >= D:
                idx_sel = np.argsort(g_abs)[::-1]
            else:
                idx_sel = np.argpartition(g_abs, -k)[-k:]
                idx_sel = idx_sel[np.argsort(g_abs[idx_sel])[::-1]]
            idx_sel = idx_sel.astype(np.int64)

        # gradient sign step on active coords only
        step = np.zeros(D, dtype=np.float32)
        sgn = np.sign(g).astype(np.float32)
        step[idx_sel] = direction * alpha * sgn[idx_sel]

        delta = delta + step
        delta = _project(delta, idx_sel)

    x_adv = (x0 + delta).astype(np.float32)
    return x_adv, idx_sel, grad_last


def attack_and_plot_pgd_topk_v3(
    pe_path,
    models_dir,
    k: int = 200,
    eps: float = _DEFAULT_EPS,
    alpha: float = 0.01,
    steps: int = 20,
    family: str | None = None,          # 'all','byte_hist','byte_entropy','generic','pe_only'
    plot_path: str = "v3_perturbation_pgd_topk.png",
    save_npy: bool = True,
    reselection: bool = False,
):
    """
    End-to-end masked-PGD (sparse top-k) demo with summary + plot.
    """
    # artifacts + features
    model, mean, var, cfg = load_artifacts(models_dir)
    vec = extract_v3_features(pe_path)
    D = cfg["input_dim"]
    if vec.shape[0] != D:
        raise ValueError(f"Feature dim mismatch: got {vec.shape[0]}, expected {D}")
    x0 = standardize(vec, mean, var).astype(np.float32)

    # baseline
    logit0, prob0 = _predict_logit_prob_from_stdvec(model, x0)

    # family mask
    m = _family_mask_v3(family or "all", D=D)

    # run masked-PGD
    xk, idx_sel, grad_last = pgd_topk_attack_v3(
        model, x0, k=k, target_prob=0.0, eps=eps, alpha=alpha, steps=steps,
        mask=m, random_start=True, reselection=reselection
    )

    # after score
    logit1, prob1 = _predict_logit_prob_from_stdvec(model, xk)

    # save vectors
    if save_npy:
        np.save("original_scaled_vector_v3.npy", x0)
        np.save("adversarial_scaled_vector_v3.npy", xk)

    # deltas + counts
    delta = (xk - x0).astype(np.float32)
    abs_delta = np.abs(delta)
    changed = int(np.count_nonzero(abs_delta > 0))
    eps_hits = int(np.count_nonzero(np.isclose(abs_delta[idx_sel], eps, atol=1e-6)))

    # rank selected by |grad_last|
    rk = idx_sel[np.argsort(np.abs(grad_last[idx_sel]))[::-1]]

    # scatter plot on selected coords (clear needles)
    plt.figure(figsize=(12, 4))
    plt.scatter(idx_sel, delta[idx_sel], s=8)
    plt.title(f"PGD top-{k} (steps={steps}, α={alpha}, ε={eps}, family={family or 'all'}, reselection={reselection})")
    plt.xlabel("Feature index (0…2567)")
    plt.ylabel("Δ (standardized)")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.savefig(plot_path); plt.close()

    # top-5 report
    top = []
    for j in rk[:5]:
        top.append({
            "index": int(j),
            "delta": float(delta[j]),
            "grad_abs": float(abs(grad_last[j])),
            "family": feature_block_v3(int(j)),
        })

    flipped = prob1 < 0.5

    print(f"\nFile: {pe_path}")
    print(f"Baseline: logit={logit0:.4f}, prob={prob0:.4f}")
    print(f"After PGD top-{k} (steps={steps}, α={alpha}, ε={eps}, family={family or 'all'}, reselection={reselection}): "
          f"logit={logit1:.4f}, prob={prob1:.4f}  -> flipped={flipped}")
    print(f"Changed features (nonzero Δ): {changed:,} (selected top-k={len(idx_sel)})")
    print(f"Of selected, |Δ| ≈ ε: {eps_hits}/{len(idx_sel)}")
    for i, t in enumerate(top, 1):
        print(f"  {i}. idx={t['index']:4d}  Δ={t['delta']:+.4f}  |grad|={t['grad_abs']:.4e}  fam={t['family']}")
    print(f"Plot saved: {plot_path}\n")

    return {
        "file": pe_path,
        "epsilon": float(eps),
        "alpha": float(alpha),
        "steps": int(steps),
        "k": int(k),
        "family": family or "all",
        "reselection": bool(reselection),
        "before": {"logit": float(logit0), "prob": float(prob0)},
        "after":  {"logit": float(logit1), "prob": float(prob1)},
        "flipped": flipped,
        "selected_indices": idx_sel.tolist(),
        "changed_features": changed,
        "eps_hits_selected": int(eps_hits),
        "plot_path": plot_path,
        "top_changes": top,
    }

# -------- Sparse L0 flip (exponential + binary search) for v3 --------

import torch.nn.functional as F

def _rank_by_effect_v3(grad: torch.Tensor,
                       lower: torch.Tensor | None = None,
                       upper: torch.Tensor | None = None) -> torch.Tensor:
    g = grad.abs()
    if lower is not None and upper is not None:
        span = (upper - lower).abs().clamp_min(1e-12)
        g = g * span
    return torch.argsort(g, descending=True)

def _forward_logit_std(model, x_std_vec: torch.Tensor) -> torch.Tensor:
    x = x_std_vec if x_std_vec.ndim == 2 else x_std_vec.unsqueeze(0)
    return model(x).view(())

def _grad_at_std(model, x_std_vec: torch.Tensor, target_prob: float) -> torch.Tensor:
    x = x_std_vec.detach().clone().requires_grad_(True)
    y_t = torch.tensor(float(target_prob), dtype=x.dtype, device=x.device).view(())
    logit = _forward_logit_std(model, x)
    loss = F.binary_cross_entropy_with_logits(logit, y_t)
    model.zero_grad(set_to_none=True)
    g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach()
    return g

def _project_box_std(x_std_vec, lower_std=None, upper_std=None):
    if lower_std is None or upper_std is None:
        return x_std_vec
    return torch.max(torch.min(x_std_vec, upper_std), lower_std)

def _optimize_on_mask_v3(
    model,
    x0_std: torch.Tensor,
    mask_bool: torch.Tensor,
    target_prob: float,
    inner_steps: int = 200,
    inner_lr: float = 0.05,
    lower_std: torch.Tensor | None = None,
    upper_std: torch.Tensor | None = None,
    patience: int = 3,
    random_restarts: int = 3,
    margin: float = 0.499,              # <-- NEW: honor margin inside optimizer
):
    """
    Gradient-descent optimizer restricted to 'mask_bool'.
    Returns (x_best, success_bool) where success means the requested margin
    was achieved (p <= margin for target=0, or p >= 1 - margin for target=1).
    """
    device = x0_std.device

    def project(x_std_vec):
        if lower_std is None or upper_std is None:
            return x_std_vec
        return torch.max(torch.min(x_std_vec, upper_std), lower_std)

    def success(p: float) -> bool:
        if target_prob <= 0.5:   # benign target
            return p <= margin
        else:                    # malicious target
            return p >= (1.0 - margin)

    # If already successful at start, return immediately
    with torch.no_grad():
        logit0 = _forward_logit_std(model, x0_std)
        p0 = float(torch.sigmoid(logit0).item())
        if success(p0):
            return x0_std.clone(), True

    best_x = x0_std.clone()
    best_val = float("inf")

    for _ in range(random_restarts):
        x = x0_std.clone()
        # tiny randomization on editable coords
        x += (torch.rand_like(x) - 0.5) * 0.02 * mask_bool.float()
        x = project(x)

        no_improve = 0
        for _ in range(inner_steps):
            x = x.detach().requires_grad_(True)
            logit = _forward_logit_std(model, x)

            # check success against margin (NOT 0.5)
            p = float(torch.sigmoid(logit).item())
            if success(p):
                return x.detach(), True

            y_t = torch.tensor(float(target_prob), dtype=logit.dtype, device=device)
            loss = F.binary_cross_entropy_with_logits(logit, y_t)

            model.zero_grad(set_to_none=True)
            g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach()

            # take a step ONLY on the mask
            step = torch.zeros_like(x)
            step[mask_bool] = -inner_lr * g[mask_bool]
            x = x.detach() + step
            x = project(x)

            val = float(loss.item())
            if val + 1e-6 < best_val:
                best_val, best_x = val, x.clone()
                no_improve = 0
            else:
                no_improve += 1
                if no_improve >= patience:
                    no_improve = 0  # (you could also break here to be stricter)

        # after inner loop, one more success check on best_x
        with torch.no_grad():
            logit_best = _forward_logit_std(model, best_x)
            p_best = float(torch.sigmoid(logit_best).item())
            if success(p_best):
                return best_x, True

    return best_x, False

# --- REPLACE the calls inside sparse_l0_flip_v3 to pass margin through ---
def sparse_l0_flip_v3(
    model,
    x0_std: torch.Tensor,              # [D], standardized
    target_prob: float,                # 0.0 or 1.0
    k_min: int = 1,
    k_max: int | None = None,
    max_outer_steps: int = 20,
    inner_steps: int = 200,
    inner_lr: float = 0.05,
    lower_std: torch.Tensor | None = None,
    upper_std: torch.Tensor | None = None,
    random_restarts: int = 3,
    patience: int = 3,
    editable_mask: torch.Tensor | None = None,  # bool [D]
    margin: float = 0.499,            # stop only when prob_after reaches margin
    report_tol: float = 1e-8,         # nonzero Δ threshold for changed-idx detection
):
    """
    Find smallest k (L0) that flips under standardized-space updates.
    Returns: (x_best, k_found, flipped_bool, changed_idx_np, final_mask_np)
    """
    device = x0_std.device
    D = x0_std.numel()
    if k_max is None: k_max = D
    if editable_mask is None:
        editable_mask = torch.ones(D, dtype=torch.bool, device=device)
    elif editable_mask.numel() != D:
        raise ValueError("editable_mask has wrong length")

    def _is_success(x_vec: torch.Tensor) -> bool:
        with torch.no_grad():
            p = float(torch.sigmoid(_forward_logit_std(model, x_vec)).item())
            return (p <= margin) if target_prob <= 0.5 else (p >= 1.0 - margin)

    # Already at target margin?
    if _is_success(x0_std):
        return (x0_std.clone(), 0, True,
                np.array([], dtype=int),
                editable_mask.detach().cpu().numpy().astype(bool))

    def grad_rank(x_std):
        g = _grad_at_std(model, x_std, target_prob)
        order = _rank_by_effect_v3(g)  # |grad|
        return order, g

    # 1) Exponential search for some successful k
    x_base = x0_std.clone()
    k = k_min
    last_success = None  # (x_success, k_success, mask_success)
    for _ in range(max_outer_steps):
        order, _ = grad_rank(x_base)  # (ranking at start; you could also use x0_std)
        mask = torch.zeros(D, dtype=torch.bool, device=device)
        chosen = 0
        for idx in order.tolist():
            if editable_mask[idx]:
                mask[idx] = True
                chosen += 1
                if chosen >= k:
                    break
        if chosen == 0:
            return (x0_std.clone(), None, False,
                    np.array([], dtype=int),
                    editable_mask.detach().cpu().numpy().astype(bool))

        x_k, ok = _optimize_on_mask_v3(
            model, x0_std, mask, target_prob,
            inner_steps=inner_steps, inner_lr=inner_lr,
            lower_std=lower_std, upper_std=upper_std,
            patience=patience, random_restarts=random_restarts,
            margin=margin,                             # <-- pass margin through
        )
        if ok and _is_success(x_k):
            last_success = (x_k.clone(), chosen, mask.clone())
            break
        k = min(k * 2, k_max)
        if k == (k // 2):  # guard
            break

    if last_success is None:
        return (x0_std.clone(), None, False,
                np.array([], dtype=int),
                editable_mask.detach().cpu().numpy().astype(bool))

    # 2) Binary search to minimal k (always starting from x0_std)
    x_best, high, mask_best = last_success
    low = 1
    while low < high:
        mid = (low + high) // 2
        order, _ = grad_rank(x0_std)
        mask = torch.zeros(D, dtype=torch.bool, device=device)
        chosen = 0
        for idx in order.tolist():
            if editable_mask[idx]:
                mask[idx] = True
                chosen += 1
                if chosen >= mid:
                    break
        if chosen == 0:
            break

        x_mid, ok = _optimize_on_mask_v3(
            model, x0_std, mask, target_prob,
            inner_steps=inner_steps, inner_lr=inner_lr,
            lower_std=lower_std, upper_std=upper_std,
            patience=patience, random_restarts=random_restarts,
            margin=margin,                         # <-- pass margin through
        )
        if ok and _is_success(x_mid):
            x_best, high, mask_best = x_mid, chosen, mask.clone()
        else:
            low = mid + 1

    # Report changed indices (within the final mask)
    delta = (x_best.detach().cpu().numpy() - x0_std.detach().cpu().numpy()).astype(np.float32)
    mask_np = mask_best.detach().cpu().numpy().astype(bool)
    changed_idx = np.where((np.abs(delta) > report_tol) & mask_np)[0].astype(int)

    return x_best, int(high), True, changed_idx, mask_np


def run_sparse_flip_demo_v3(
    pe_path: str,
    models_dir: str,
    k_min: int = 1,
    inner_steps: int = 200,
    inner_lr: float = 0.05,
    family: str | None = None,                # 'all','byte_hist','byte_entropy','generic','pe_only'
    allowed_idx: list[int] | np.ndarray | None = None,
    exclude_idx: list[int] | np.ndarray | None = None,
    margin: float = 0.499,
    save_npy: bool = True,
    # NEW pass-throughs:
    random_restarts: int = 3,
    patience: int = 3,
    max_outer_steps: int = 20,
):
    model, mean, var, cfg = load_artifacts(models_dir)
    vec = extract_v3_features(pe_path)
    D = cfg["input_dim"]; assert vec.shape[0] == D
    x0 = standardize(vec, mean, var).astype(np.float32)
    x0_t = torch.from_numpy(x0)

    logit0 = _forward_logit_std(model, x0_t)
    prob0 = float(torch.sigmoid(logit0).item())
    cur_label = int(prob0 >= 0.5)
    target_prob = float(1 - cur_label)

    editable_np = build_editable_mask(D=D, family=family, allowed_idx=allowed_idx, exclude_idx=exclude_idx)
    editable_mask = torch.tensor(editable_np, dtype=torch.bool, device=x0_t.device)

    x_best, k_found, flipped, changed_idx, final_mask = sparse_l0_flip_v3(
        model=model,
        x0_std=x0_t,
        target_prob=target_prob,
        k_min=k_min,
        k_max=D,
        max_outer_steps=max_outer_steps,     # <= NEW
        inner_steps=inner_steps,
        inner_lr=inner_lr,
        lower_std=None, upper_std=None,
        random_restarts=random_restarts,     # <= NEW
        patience=patience,                   # <= NEW
        editable_mask=editable_mask,
        margin=margin,
    )

    prob1 = float(torch.sigmoid(_forward_logit_std(model, x_best)).item())

    if save_npy:
        np.save("original_scaled_vector_v3.npy", x0)
        np.save("perturbed_scaled_vector_v3.npy", x_best.detach().cpu().numpy())

    print(f"\nFile: {pe_path}")
    print(f"Baseline prob={prob0:.4f}, label={cur_label}, target={int(target_prob)}")
    print(f"[Scaled space] flipped={flipped}, k={k_found}, prob_after={prob1:.4f} (margin≤{margin})")
    print(f"Changed features inside final k-mask (count={len(changed_idx)}): {changed_idx.tolist()[:50]}{' ...' if len(changed_idx)>50 else ''}")

    return {
        "file": pe_path,
        "before_prob": prob0,
        "after_prob": prob1,
        "target": int(target_prob),
        "k_found": k_found,
        "flipped": flipped,
        "changed_idx": changed_idx.tolist(),
        "mask_family": family or "all",
        "allowed_idx": (np.asarray(allowed_idx).astype(int).tolist() if allowed_idx is not None else None),
        "exclude_idx": (np.asarray(exclude_idx).astype(int).tolist() if exclude_idx is not None else None),
        "margin": margin,
        "random_restarts": random_restarts,
        "patience": patience,
        "max_outer_steps": max_outer_steps,
    }


#################################################################################
# ===================== Overlay (addition-only) demo for v3 =====================

from dataclasses import dataclass
from typing import Optional, Sequence, Tuple

@dataclass
class OverlayConfig:
    # core
    use_overlay_loop: bool = True
    pe_in_path: str = "toy_malware.exe"
    pe_out_path: str = "demo_patch.exe"
    k_top: int = 5                     # top-K indices to push per step (addition-only)
    chunk_size_bytes: int = 1024       # bytes per iteration to append
    max_total_append: int = 32_768     # total overlay budget
    target_threshold: float = 0.45     # stop once p<=threshold (for target=0); margin below 0.5
    patience_steps: int = 4            # early stop if no improvement
    min_improve: float = 1e-3          # minimum p-change to count as improvement
    shrink_to_fit: bool = True         # binary search the last accepted chunk smaller
    seed: Optional[int] = 1337         # None disables seeding

    # feature selection constraints
    family: str = "byte_hist"          # 'all','byte_hist','byte_entropy','generic','pe_only'
    allowed_idx: Optional[Sequence[int]] = None
    exclude_idx: Optional[Sequence[int]] = None

def _decode_byte_entropy_index_v3(i: int) -> Tuple[int, int]:
    """Map indices 256..511 -> (entropy_bin 0..15, nibble_bin 0..15)."""
    pos = i - 256
    return pos // 16, pos % 16

def _append_overlay_file(in_path: str, overlay: bytes, out_path: str):
    with open(in_path, "rb") as f:
        orig = f.read()
    with open(out_path, "wb") as f:
        f.write(orig + overlay)

def _craft_overlay_for_feature_v3(idx: int,
                                  total_len: int,
                                  rng: np.random.Generator,
                                  grad_vec=None) -> Optional[bytes]:
    """
    Addition-only overlay synthesis to push feature 'idx' upward.
    Currently supports:
      - byte histogram (0..255)
      - byte-entropy histogram (256..511)
    Returns bytes or None if unsupported family index.
    """
    def nonprint_pool(exclude=set()):
        return [x for x in range(0x00, 0x100) if x not in exclude and not (0x20 <= x <= 0x7E)]
    def uniform_bytes(choices, length):
        choices = np.fromiter(choices, dtype=np.uint8)
        probs = np.full(choices.size, 1.0 / choices.size)
        return bytes(rng.choice(choices, size=length, p=probs))

    # 0) BYTE HISTOGRAM (0..255): add mostly that byte + a small non-printable mix
    if 0 <= idx <= 255:
        b = idx
        core = bytes([b]) * int(total_len * 0.9)
        pool = nonprint_pool(exclude={b}) or [0x80, 0x90, 0xA0, 0xB0, 0xC0, 0xD0, 0xE0, 0xF0]
        rest = bytes(rng.choice(np.array(pool, dtype=np.uint8), size=total_len - len(core)))
        return core + rest

    # 1) BYTE-ENTROPY HISTOGRAM (256..511): add bytes targeting a chosen nibble/entropy bin
    if 256 <= idx <= 511:
        ebin, nb = _decode_byte_entropy_index_v3(idx)
        lo = nb << 4
        hi = (nb << 4) | 0x0F
        nibble_pool = list(range(lo, hi + 1))
        if ebin <= 3:
            # low-entropy: repeated single non-printable byte in the nibble
            b = lo if not (0x20 <= lo <= 0x7E) else (lo | 0x08)
            return bytes([b]) * total_len
        else:
            # higher-entropy: random within nibble
            return uniform_bytes(nibble_pool, total_len)

    return None  # unsupported index for addition-only overlay

def _choose_topK_addition_only(
    model,
    x_scaled_vec: torch.Tensor,  # shape [1,D] or [D]
    target: int,                 # 0 or 1
    K: int,
    allowed_mask_t: torch.Tensor # bool [D]
):
    """
    Select indices where increasing the feature reduces loss (grad < 0),
    ranked by |grad| within allowed_mask. Always returns +1 signs.
    """
    x = x_scaled_vec if x_scaled_vec.ndim == 2 else x_scaled_vec[None, :]
    x = x.detach().clone().requires_grad_(True)

    logit = model(x).reshape(1)
    y_t = torch.tensor([float(target)], device=logit.device, dtype=logit.dtype)
    loss = F.binary_cross_entropy_with_logits(logit, y_t)

    model.zero_grad(set_to_none=True)
    g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach().squeeze(0)  # [D]
    g_abs = g.abs()

    # suppress disallowed
    minus_inf = torch.full_like(g_abs, float("-inf"))
    score = torch.where(allowed_mask_t, g_abs, minus_inf)
    order = torch.argsort(score, descending=True).cpu().tolist()

    pos_idxs = []
    g_np = g.cpu().numpy()
    allow_np = allowed_mask_t.cpu().numpy()

    for i in order:
        if i < 0 or not allow_np[i]:
            continue
        if g_np[i] < 0:   # increasing feature reduces loss
            pos_idxs.append(i)
            if len(pos_idxs) >= K:
                break

    # fill remaining with next-best allowed (still +1 step)
    if len(pos_idxs) < K:
        for i in order:
            if i in pos_idxs or i < 0 or not allow_np[i]:
                continue
            pos_idxs.append(i)
            if len(pos_idxs) >= K: break

    signs = np.ones(len(pos_idxs), dtype=int)  # addition-only policy
    return np.array(pos_idxs, dtype=int), signs, g_np

def run_overlay_closed_loop_v3(
    config: OverlayConfig,
    models_dir: Optional[str] = None,
    model: Optional[nn.Module] = None,
    mean: Optional[np.ndarray] = None,
    var: Optional[np.ndarray] = None,
    device: str | torch.device = "cpu",
):
    """
    Addition-only overlay loop in file space with safe candidate handling:
      - recomputes gradients on the current best file each iteration
      - writes candidates to PE_OUT.cand (never overwrites best directly)
      - accepts via os.replace(...) only if improvement clears min_improve
      - otherwise line-searches smaller chunks (1/2,1/4,1/8,1/16)
      - optional shrink-to-fit after first successful flip
    Returns a dict with summary stats and a short 'history' list.
    """
    import os, random

    if not config.use_overlay_loop:
        return {"skipped": True}

    # seeding
    if config.seed is not None:
        random.seed(config.seed)
        np.random.seed(config.seed)
        try:
            torch.manual_seed(config.seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(config.seed)
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False
        except Exception:
            pass
    rng = np.random.default_rng(config.seed if config.seed is not None else None)

    # model + stats
    if model is None or mean is None or var is None:
        if not models_dir:
            raise ValueError("Provide `models_dir` or explicit (model, mean, var).")
        model, mean, var, _ = load_artifacts(models_dir)
    model = model.to(device).eval()
    mean = np.asarray(mean, dtype=np.float32)
    var  = np.asarray(var,  dtype=np.float32)

    # editable mask = family ∩ allowed \ exclude
    D = mean.shape[0]
    mask_np = build_editable_mask(D=D, family=config.family,
                                  allowed_idx=config.allowed_idx,
                                  exclude_idx=config.exclude_idx)
    allowed_mask_t = torch.tensor(mask_np, dtype=torch.bool, device=device)

    # helpers
    def extract_scaled_from_exe_v3(
            pe_path: str,
            models_dir: str | None = None,
            mean: np.ndarray | None = None,
            var: np.ndarray | None = None,
            device: str | torch.device = "cpu",
    ) -> torch.Tensor:
        if mean is None or var is None:
            if not models_dir:
                raise ValueError("Provide `models_dir` or explicit `mean` and `var`.")
            _, mean, var, _ = load_artifacts(models_dir)
        else:
            mean = np.asarray(mean, dtype=np.float32)
            var = np.asarray(var, dtype=np.float32)

        vec = extract_v3_features(pe_path)  # (2568,)
        x_scaled = (vec.astype(np.float32) - mean) / np.sqrt(var)
        return torch.tensor(x_scaled[None, :], dtype=torch.float32, device=device)

    def _scaled_from_path(path: str) -> torch.Tensor:
        x = extract_scaled_from_exe_v3(path, mean=mean, var=var, device=device)  # [1,D]
        return x

    def _predict_prob(path: str) -> float:
        with torch.no_grad():
            return float(torch.sigmoid(model(_scaled_from_path(path)).reshape(())))

    # target: flip current prediction
    p0 = _predict_prob(config.pe_in_path)
    target = 0 if p0 >= 0.5 else 1

    best_prob = p0
    best_path = config.pe_in_path
    appended_total = 0
    patience = 0

    # Remember info needed for shrink-to-fit (base file before last accept)
    last_accept_info = None  # (base_before_chunk_path, idxs, signs, grad_vec, chunk_size)

    # candidate and scratch names (do not collide with best_path)
    cand_path_base = config.pe_out_path + ".cand"
    sub_path_base  = config.pe_out_path + ".sub"
    shrink_tmp     = config.pe_out_path + ".shrink_tmp"

    history = []  # list of dicts: {"chunk":N, "k":K, "p_before":..., "p_after":..., "accepted":bool}

    def flipped_safely(prob: float) -> bool:
        return prob <= config.target_threshold if target == 0 else prob >= (1.0 - config.target_threshold)

    while appended_total < config.max_total_append and not flipped_safely(best_prob):
        # recompute gradient on current best
        x_scaled_cur = _scaled_from_path(best_path)
        x = x_scaled_cur.detach().clone().requires_grad_(True)
        logit = model(x).reshape(1)
        y_t = torch.tensor([float(target)], device=logit.device, dtype=logit.dtype)
        loss = F.binary_cross_entropy_with_logits(logit, y_t)
        model.zero_grad(set_to_none=True)
        g = torch.autograd.grad(loss, x, retain_graph=False, create_graph=False)[0].detach().squeeze(0)  # [D]
        g_abs = g.abs()

        # top-K addition-only indices (grad<0) under mask
        minus_inf = torch.full_like(g_abs, float("-inf"))
        score = torch.where(allowed_mask_t, g_abs, minus_inf)
        order = torch.argsort(score, descending=True).cpu().tolist()

        pos_idxs = []
        g_np = g.cpu().numpy()
        allow_np = allowed_mask_t.detach().cpu().numpy()

        for i in order:
            if i < 0 or not allow_np[i]:
                continue
            if g_np[i] < 0:  # increasing feature reduces loss
                pos_idxs.append(i)
                if len(pos_idxs) >= config.k_top:
                    break
        if len(pos_idxs) == 0:
            print("[Overlay] No editable indices available; stopping.")
            break

        idxs = np.array(pos_idxs, dtype=int)
        # allocate bytes across idxs proportional to |grad|
        weights = np.abs(g_np[idxs]); weights = weights / (weights.sum() + 1e-12)

        def build_overlay(total_bytes: int) -> bytes:
            lengths = (weights * total_bytes).astype(int)
            gap = total_bytes - int(lengths.sum())
            if gap > 0:
                lengths[0] += gap
            parts = []
            for i, L in zip(idxs, lengths):
                if L <= 0: continue
                blob = _craft_overlay_for_feature_v3(i, total_len=int(L), rng=rng, grad_vec=g_np)
                if blob: parts.append(blob)
            return b"".join(parts)

        # ---- main candidate (full chunk) ----
        overlay = build_overlay(config.chunk_size_bytes)
        if not overlay:
            print("[Overlay] overlay crafter returned empty; stopping.")
            break

        # IMPORTANT: write to a CANDIDATE file, never overwrite best in place
        cand_path = cand_path_base
        _append_overlay_file(best_path, overlay, cand_path)
        p_after = _predict_prob(cand_path)

        print(f"[Overlay] step chunk={config.chunk_size_bytes}, K={len(idxs)}, "
              f"p: {best_prob:.4f} -> {p_after:.4f}, idxs={idxs.tolist()}")

        improved = (target == 0 and p_after < best_prob - config.min_improve) or \
                   (target == 1 and p_after > best_prob + config.min_improve)

        if improved:
            print(f"[Overlay] accepted chunk={config.chunk_size_bytes}")
            # publish candidate as new best
            os.replace(cand_path, config.pe_out_path)
            # record shrink base BEFORE acceptance (current best_path)
            last_accept_info = (best_path, idxs, np.ones(len(idxs), int), g_np, config.chunk_size_bytes)
            best_path = config.pe_out_path
            best_prob = p_after
            appended_total += config.chunk_size_bytes
            patience = 0
            history.append({"chunk": config.chunk_size_bytes, "k": len(idxs),
                            "p_before": float(best_prob), "p_after": float(p_after), "accepted": True})
            continue
        else:
            # try smaller sub-chunks before giving up
            accepted = False
            for sub in [config.chunk_size_bytes // 2,
                        config.chunk_size_bytes // 4,
                        config.chunk_size_bytes // 8,
                        config.chunk_size_bytes // 16]:
                if not sub or sub < 128:
                    continue
                overlay_sub = build_overlay(sub)
                if not overlay_sub:
                    continue
                sub_path = sub_path_base
                _append_overlay_file(best_path, overlay_sub, sub_path)
                p_sub = _predict_prob(sub_path)
                print(f"[Overlay]   sub-chunk={sub}: {best_prob:.4f} -> {p_sub:.4f}")
                improved_sub = (target == 0 and p_sub < best_prob - config.min_improve) or \
                               (target == 1 and p_sub > best_prob + config.min_improve)
                if improved_sub:
                    print(f"[Overlay] accepted sub-chunk={sub}")
                    os.replace(sub_path, config.pe_out_path)  # accept sub as new best
                    last_accept_info = (best_path, idxs, np.ones(len(idxs), int), g_np, sub)
                    best_path = config.pe_out_path
                    best_prob = p_sub
                    appended_total += sub
                    patience = 0
                    accepted = True
                    history.append({"chunk": int(sub), "k": len(idxs),
                                    "p_before": float(best_prob), "p_after": float(p_sub), "accepted": True})
                    # clean full-candidate if present
                    try: os.remove(cand_path)
                    except OSError: pass
                    break
                else:
                    try: os.remove(sub_path)
                    except OSError: pass

            if not accepted:
                # neither full nor sub-chunks improved enough; discard candidate safely
                try: os.remove(cand_path)
                except OSError: pass
                patience += 1
                history.append({"chunk": config.chunk_size_bytes, "k": len(idxs),
                                "p_before": float(best_prob), "p_after": float(p_after), "accepted": False})
                if patience >= config.patience_steps:
                    print("[Overlay] patience exceeded; stopping.")
                    break

    # ---- shrink-to-fit (optional) ----
    def flipped_safely(prob: float) -> bool:
        return prob <= config.target_threshold if target == 0 else prob >= (1.0 - config.target_threshold)

    if config.shrink_to_fit and last_accept_info is not None and flipped_safely(best_prob):
        base_path, idxs, signs, g_np, last_len = last_accept_info
        low, high = 1, last_len
        best_keep_prob = best_prob
        best_keep_path = best_path
        while low < high:
            mid = (low + high) // 2
            # rebuild from the base (pre-accept) with a smaller chunk
            lengths = (np.abs(g_np[idxs]) / (np.abs(g_np[idxs]).sum() + 1e-12) * mid).astype(int)
            gap = mid - int(lengths.sum())
            if gap > 0: lengths[0] += gap
            parts = []
            for i, L in zip(idxs, lengths):
                if L <= 0: continue
                blob = _craft_overlay_for_feature_v3(int(i), total_len=int(L), rng=rng, grad_vec=g_np)
                if blob: parts.append(blob)
            overlay_small = b"".join(parts)
            _append_overlay_file(base_path, overlay_small, shrink_tmp)
            p_mid = _predict_prob(shrink_tmp)
            if flipped_safely(p_mid):
                best_keep_prob = p_mid
                best_keep_path = shrink_tmp
                high = mid
            else:
                try: os.remove(shrink_tmp)
                except OSError: pass
                low = mid + 1

        if best_keep_path != best_path:
            try:
                if os.path.exists(best_path): os.remove(best_path)
            except OSError:
                pass
            os.replace(best_keep_path, config.pe_out_path)
            best_path = config.pe_out_path
            appended_total = appended_total - last_len + high
            best_prob = best_keep_prob
            print(f"[Overlay] shrink-to-fit accepted, final_chunk={high}, prob={best_prob:.4f}")

    return {
        "in_path": config.pe_in_path,
        "out_path": best_path,
        "baseline_prob": p0,
        "final_prob": best_prob,
        "total_appended": int(appended_total),
        "flipped": flipped_safely(best_prob),
        "target": target,
        "k_top": int(config.k_top),
        "chunk_size": int(config.chunk_size_bytes),
        "family": config.family,
        "allowed_idx": (list(config.allowed_idx) if config.allowed_idx is not None else None),
        "exclude_idx": (list(config.exclude_idx) if config.exclude_idx is not None else None),
        "history": history,
    }

# =================== end overlay helpers (v3) ===================

# --- imports this block needs (add if not already present at top of file) ---
import numpy as np
import torch
import torch.nn.functional as F

# ---------- Public: extract standardized v3 features from a PE path ----------
def extract_scaled_from_exe_v3(
    pe_path: str,
    models_dir: str | None = None,
    mean: np.ndarray | None = None,
    var:  np.ndarray | None = None,
    device: str | torch.device = "cpu",
) -> torch.Tensor:
    """
    Extract EMBER v3 (thrember) features from a PE, then standardize with your saved mean/var.
    Returns a tensor of shape [1, D] on `device`.
    Supply either (mean,var) directly or models_dir to auto-load artifacts.
    """
    if mean is None or var is None:
        if not models_dir:
            raise ValueError("Provide `models_dir` or explicit `mean` and `var`.")
        _, mean, var, _ = load_artifacts(models_dir)

    mean = np.asarray(mean, dtype=np.float32)
    std  = np.sqrt(np.asarray(var, dtype=np.float32)).clip(1e-12)

    vec = extract_v3_features(pe_path).astype(np.float32)   # (D,)
    x_scaled = (vec - mean) / std                           # (D,)
    return torch.tensor(x_scaled[None, :], dtype=torch.float32, device=device)

# ---------- Normalization-aware selection for histogram families ----------
def _family_slices_v3(D: int):
    bh = slice(0, min(256, D))        # byte histogram
    be = slice(256, min(512, D))      # byte-entropy histogram
    return bh, be

def _normaware_scores_for_histograms_v3(
    x_scaled: torch.Tensor,          # [1, D] standardized
    grad_scaled: torch.Tensor,       # [D]   dL/d(standardized x)
    mean: np.ndarray,
    var:  np.ndarray,
    family: str = "byte_hist",       # "byte_hist" | "byte_entropy" | "all"
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute s_j = g_raw[j] - <g_raw, h> for bins in the chosen histogram family/families,
    where h is the *unstandardized* current feature vector (histogram); +inf elsewhere.
    Returns (s, craftable_mask, h_raw) as numpy arrays, each [D].
    """
    x = x_scaled.detach().reshape(1, -1)
    D = x.shape[1]
    device = x.device

    mu_t  = torch.from_numpy(np.asarray(mean, dtype=np.float32)).to(device)
    std_t = torch.from_numpy(np.sqrt(np.asarray(var, dtype=np.float32))).to(device).clamp_min(1e-12)

    h      = (x.squeeze(0) * std_t) + mu_t   # unstandardized features
    g_raw  = grad_scaled / std_t             # chain rule back to raw feature space

    bh, be = _family_slices_v3(D)
    fam = (family or "byte_hist").lower()
    use_bh = fam in ("byte_hist", "all")
    use_be = fam in ("byte_entropy", "all")

    s = torch.full((D,), float("inf"), device=device)
    craft = torch.zeros(D, dtype=torch.bool, device=device)

    if use_bh and bh.stop > bh.start:
        dot_bh = (g_raw[bh] * h[bh]).sum()
        s[bh] = g_raw[bh] - dot_bh
        craft[bh] = True

    if use_be and be.stop > be.start:
        dot_be = (g_raw[be] * h[be]).sum()
        s[be] = g_raw[be] - dot_be
        craft[be] = True

    return s.detach().cpu().numpy(), craft.detach().cpu().numpy(), h.detach().cpu().numpy()

def _normaware_topk_indices_v3(
    x_scaled: torch.Tensor,             # [1, D]
    grad_scaled: torch.Tensor,          # [D]
    mean: np.ndarray, var: np.ndarray,  # [D], [D]
    family: str,
    allowed_mask: np.ndarray | torch.Tensor,  # [D] bool
    k_top: int,
) -> tuple[np.ndarray, np.ndarray]:
    """
    Keep ONLY bins with s_j < 0 (addition-only helps), rank by s (most negative first),
    and return up to k_top indices plus their scores.
    """
    allow_np = allowed_mask.detach().cpu().numpy().astype(bool) if isinstance(allowed_mask, torch.Tensor) \
               else np.asarray(allowed_mask, dtype=bool)

    s_all, craftable, _ = _normaware_scores_for_histograms_v3(
        x_scaled=x_scaled, grad_scaled=grad_scaled, mean=mean, var=var, family=family
    )

    sel = craftable & allow_np
    s_sel = s_all.copy()
    s_sel[~sel] = np.inf

    neg = np.where(s_sel < 0.0)[0]
    if neg.size == 0:
        return np.array([], dtype=int), np.array([], dtype=float)

    order = neg[np.argsort(s_sel[neg])]           # ascending s: most negative first
    top   = order[: min(k_top, order.size)]
    return top, s_sel[top]

# ---------- Public: normalization-aware FGSM top-k (feature space only) ----------
def fgsm_linf_topk_normaware_v3(
    x_scaled: torch.Tensor,          # [1, D]
    model: torch.nn.Module,
    mean: np.ndarray, var: np.ndarray,
    epsilon: float = 0.05,
    k_top: int = 200,
    target: int | None = None,
    family: str = "byte_hist",       # "byte_hist" | "byte_entropy" | "all"
    allowed_idx=None, exclude_idx=None,
) -> tuple[torch.Tensor, dict]:
    """
    One-step, addition-only, normalization-aware FGSM on histogram families.
    Moves only standardized coords with s_j<0 by +epsilon (others unchanged).
    """
    device = next(model.parameters()).device
    x = x_scaled.detach().clone()
    if x.ndim == 1:
        x = x.unsqueeze(0)
    x = x.to(device)

    # Allowed mask using your existing helper
    D = x.shape[1]
    allow_np = build_editable_mask(D=D, family=family,
                                   allowed_idx=allowed_idx, exclude_idx=exclude_idx).astype(bool)

    with torch.no_grad():
        p0 = torch.sigmoid(model(x)).reshape(()).item()
    tgt = (0 if p0 >= 0.5 else 1) if target is None else int(target)

    # gradient wrt standardized x
    x_req = x.detach().requires_grad_(True)
    logit = model(x_req).reshape(1)
    y_t   = torch.tensor([float(tgt)], device=device, dtype=logit.dtype)
    loss  = F.binary_cross_entropy_with_logits(logit, y_t)
    model.zero_grad(set_to_none=True)
    g_scaled = torch.autograd.grad(loss, x_req, retain_graph=False, create_graph=False)[0].detach().squeeze(0)  # [D]

    # pick only beneficial bins
    idxs, s_vals = _normaware_topk_indices_v3(
        x_scaled=x, grad_scaled=g_scaled, mean=mean, var=var,
        family=family, allowed_mask=allow_np, k_top=k_top
    )

    if idxs.size == 0:
        return x.detach(), {
            "before_prob": p0,
            "after_prob": p0,
            "changed": 0,
            "selected_idx": [],
            "selected_s": [],
        }

    delta = torch.zeros_like(x)
    delta[0, torch.from_numpy(idxs).to(device)] = float(epsilon)  # addition-only in raw => +epsilon in standardized
    x_adv = x + delta

    with torch.no_grad():
        p1 = torch.sigmoid(model(x_adv)).reshape(()).item()

    return x_adv.detach(), {
        "before_prob": p0,
        "after_prob": p1,
        "changed": int(idxs.size),
        "selected_idx": idxs.tolist(),
        "selected_s": s_vals.tolist(),
    }

def fgsm_normaware_iter_v3(
    x_scaled: torch.Tensor,             # [1, D] standardized start point (feature space)
    model: torch.nn.Module,
    mean: np.ndarray, var: np.ndarray,
    steps: int = 10,                    # max iterations
    epsilon: float = 0.03,              # step size per iteration (standardized units)
    k_top: int = 50,                    # top-K negative-score bins each step
    family: str = "all",                # "byte_hist" | "byte_entropy" | "all"
    allowed_idx=None, exclude_idx=None,
    target: int | None = None,          # None => flip current label (0 for benign, 1 for malicious)
    margin: float | None = 0.49,        # stop once p <= margin (target=0) or p >= 1-margin (target=1); set None to disable
    epsilon_decay: float = 1.0,         # multiply epsilon after each accepted step (e.g., 0.9)
    cap_linf: float | None = None,      # optional per-coordinate cap on total delta in standardized space
    min_improve: float = 0.0,           # require >= this much improvement to accept a step
    patience: int = 3,                  # stop after this many non-improving iterations
) -> tuple[torch.Tensor, dict]:
    """
    Iterative normalization-aware FGSM in feature space (no file editing).
    Each step:
      1) gradient at current x
      2) select ONLY bins with s_j < 0 (addition-only helpful), top-K
      3) add +epsilon on those standardized dims (clip by cap_linf if provided)
      4) accept only if probability moves toward target by >= min_improve
      5) optional early stop once 'margin' reached
    Returns: (x_final, summary_dict_with_history)
    """
    device = next(model.parameters()).device
    x = x_scaled.detach().clone()
    if x.ndim == 1:
        x = x.unsqueeze(0)
    x = x.to(device)

    D = x.shape[1]
    allow_np = build_editable_mask(
        D=D, family=family, allowed_idx=allowed_idx, exclude_idx=exclude_idx
    ).astype(bool)

    # current prob + target
    with torch.no_grad():
        p0 = torch.sigmoid(model(x)).reshape(()).item()
    tgt = (0 if p0 >= 0.5 else 1) if target is None else int(target)

    def improves(p_before: float, p_after: float) -> bool:
        if tgt == 0:
            return (p_before - p_after) >= min_improve
        else:
            return (p_after - p_before) >= min_improve

    def hit_margin(p: float) -> bool:
        if margin is None:
            return False
        return (p <= margin) if tgt == 0 else (p >= (1.0 - margin))

    # optional per-coordinate cap
    accum = torch.zeros(D, dtype=torch.float32, device=device)  # total standardized delta added so far (>=0 on moved dims)

    hist = []
    best_x = x.clone()
    best_p = p0
    no_imp = 0
    eps = float(epsilon)

    for t in range(1, steps + 1):
        # gradient wrt standardized x
        x_req = best_x.detach().requires_grad_(True)
        logit = model(x_req).reshape(1)
        y_t = torch.tensor([float(tgt)], device=device, dtype=logit.dtype)
        loss = F.binary_cross_entropy_with_logits(logit, y_t)
        model.zero_grad(set_to_none=True)
        g_scaled = torch.autograd.grad(loss, x_req, retain_graph=False, create_graph=False)[0].detach().squeeze(0)  # [D]

        # select only negative-score bins
        idxs, s_vals = _normaware_topk_indices_v3(
            x_scaled=best_x, grad_scaled=g_scaled, mean=mean, var=var,
            family=family, allowed_mask=allow_np, k_top=k_top
        )
        if idxs.size == 0:
            hist.append({"step": t, "action": "stop_no_neg_bins", "prob": best_p})
            break

        # enforce per-coordinate cap if asked
        if cap_linf is not None:
            remaining = cap_linf - accum.detach().cpu().numpy()
            mask_cap = remaining[idxs] > 1e-9
            idxs = idxs[mask_cap]
            s_vals = s_vals[mask_cap]
            if idxs.size == 0:
                hist.append({"step": t, "action": "stop_cap_exhausted", "prob": best_p})
                break

        # propose step
        step = torch.zeros_like(best_x)
        idx_t = torch.from_numpy(idxs).to(device)
        step_mag = eps
        if cap_linf is not None:
            # clip step to remaining cap on each coord
            rem = torch.from_numpy((cap_linf - accum.detach().cpu().numpy()).clip(min=0.0)).to(device)
            local = torch.minimum(rem[idx_t], torch.full_like(rem[idx_t], step_mag))
            step[0, idx_t] = local
        else:
            step[0, idx_t] = step_mag

        x_cand = best_x + step

        with torch.no_grad():
            p_cand = torch.sigmoid(model(x_cand)).reshape(()).item()

        # monotonic guard: accept only if helps; otherwise try a half-step once
        accepted = False
        if improves(best_p, p_cand):
            accepted = True
        else:
            # one retry at half epsilon
            if step_mag > 1e-9:
                half_step = step.clone()
                half_step[0, idx_t] = half_step[0, idx_t] * 0.5
                x_half = best_x + half_step
                with torch.no_grad():
                    p_half = torch.sigmoid(model(x_half)).reshape(()).item()
                if improves(best_p, p_half):
                    x_cand, p_cand = x_half, p_half
                    step = half_step
                    accepted = True

        # apply decision
        hist.append({
            "step": t,
            "k_used": int(idxs.size),
            "epsilon": float(eps),
            "before_prob": float(best_p),
            "after_prob": float(p_cand),
            "accepted": bool(accepted),
            "selected_idx": idxs.tolist(),
            "selected_s": s_vals.tolist()[:10],  # keep short for logs
        })

        if accepted:
            best_x = x_cand
            best_p = p_cand
            # update accumulation & decay epsilon if requested
            accum[idx_t] += step[0, idx_t]
            if epsilon_decay != 1.0:
                eps *= float(epsilon_decay)
            no_imp = 0
            if hit_margin(best_p):
                hist.append({"step": t, "action": "hit_margin", "prob": best_p})
                break
        else:
            no_imp += 1
            if no_imp >= patience:
                hist.append({"step": t, "action": "stop_patience", "prob": best_p})
                break

    summary = {
        "before_prob": p0,
        "after_prob": best_p,
        "steps_run": len([h for h in hist if "before_prob" in h]),
        "flipped": hit_margin(best_p) if margin is not None else None,
        "history": hist,
        "final_changed_coords": int((accum > 0).sum().item()),
        "total_linf_added_max": float(accum.max().item()) if accum.numel() else 0.0,
    }
    return best_x.detach(), summary
