import torch
import torch.nn as nn
import numpy as np
import ember
import joblib
import os


# --- 1. Define the MLP Model Architecture ---
# The class definition must be available to load the model's state_dict
class MalwareMLP(nn.Module):
    def __init__(self, input_features=2381):
        super(MalwareMLP, self).__init__()
        self.layer1 = nn.Linear(input_features, 512)
        self.relu1 = nn.ReLU()
        self.layer2 = nn.Linear(512, 256)
        self.relu2 = nn.ReLU()
        self.output_layer = nn.Linear(256, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu1(self.layer1(x))
        x = self.relu2(self.layer2(x))
        x = self.sigmoid(self.output_layer(x))
        return x


def create_adversarial_vector(file_path="toy_malware.exe", epsilon=0.1):
    """
    Performs the FGSM attack on a single file's feature vector.
    """
    # --- 2. Load Model and Scaler ---
    model_path = "mlp_model.pth"
    scaler_path = "scaler.joblib"
    if not (os.path.exists(model_path) and os.path.exists(scaler_path)):
        print("Model or scaler not found. Please run train_mlp.py first.")
        return

    model = MalwareMLP()
    model.load_state_dict(torch.load(model_path))
    model.eval()
    scaler = joblib.load(scaler_path)

    # --- 3. Extract and Scale Features from the Original File ---
    if not os.path.exists(file_path):
        print(f"File '{file_path}' not found.")
        return

    with open(file_path, "rb") as f:
        bytez = f.read()

    # --- THIS IS THE FIX: Explicitly use feature_version=2 ---
    extractor = ember.PEFeatureExtractor(feature_version=2)
    # --- END OF FIX ---

    raw_features = np.array(extractor.raw_features(bytez), dtype=np.float32)
    scaled_features = scaler.transform(raw_features.reshape(1, -1))

    # Save the CORRECT original vector for analysis
    np.save('original_scaled_vector.npy', scaled_features)

    # Convert to a tensor for the attack
    x0 = torch.tensor(scaled_features, dtype=torch.float32)
    x0.requires_grad = True

    # --- 4. Get Initial Prediction ---
    with torch.no_grad():
        initial_score = model(x0).item()
    print(f"Initial classification score: {initial_score:.4f} -> {'MALICIOUS' if initial_score >= 0.5 else 'BENIGN'}")

    # --- 5. Perform the FGSM Attack ---
    print(f"Performing FGSM attack with epsilon = {epsilon}...")

    # Calculate loss and gradients
    output = model(x0)
    # We want to push the prediction towards BENIGN (0)
    loss = nn.BCELoss()(output, torch.tensor([[0.0]], dtype=torch.float32))
    model.zero_grad()
    loss.backward()

    # Collect the sign of the data gradient
    grad_sign = x0.grad.data.sign()

    # Create the perturbed tensor by subtracting the gradient (moving away from malicious)
    x_adv = x0 - epsilon * grad_sign

    # --- 6. Get Adversarial Prediction ---
    with torch.no_grad():
        adversarial_score = model(x_adv).item()
    print(
        f"Adversarial classification score: {adversarial_score:.4f} -> {'MALICIOUS' if adversarial_score >= 0.5 else 'BENIGN'}")

    if adversarial_score < 0.5:
        print("\nAttack successful! The model was fooled.")
        # Save the adversarial vector for analysis
        np.save('adversarial_scaled_vector.npy', x_adv.detach().numpy())
        print("Files 'original_scaled_vector.npy' and 'adversarial_scaled_vector.npy' saved.")
    else:
        print("\nAttack failed. Try increasing epsilon.")


if __name__ == '__main__':
    create_adversarial_vector()

# import torch
# import torch.nn as nn
# import numpy as np
# import ember
# from sklearn.preprocessing import StandardScaler
# import os
#
#
# # --- 1. Define the MLP Model Architecture ---
# # This must be the exact same architecture as the one we trained.
# class MalwareMLP(nn.Module):
#     """A simple Multi-Layer Perceptron for malware classification."""
#
#     def __init__(self, input_features=2381):
#         super(MalwareMLP, self).__init__()
#         self.layer1 = nn.Linear(input_features, 512)
#         self.relu1 = nn.ReLU()
#         self.layer2 = nn.Linear(512, 256)
#         self.relu2 = nn.ReLU()
#         self.output_layer = nn.Linear(256, 1)
#         self.sigmoid = nn.Sigmoid()
#
#     def forward(self, x):
#         x = self.relu1(self.layer1(x))
#         x = self.relu2(self.layer2(x))
#         x = self.sigmoid(self.output_layer(x))
#         return x
#
#
# def perform_attack(victim_file_path, epsilon):
#     """
#     Performs the FGSM adversarial attack on the victim file's feature vector.
#     """
#     # --- 2. Load the Scaler and Model ---
#     print("Initializing feature scaler and loading trained model...")
#     data_dir = os.path.join("data", "ember_dataset_2018_2", "ember2018")
#     X_train, y_train, _, _ = ember.read_vectorized_features(data_dir)
#     train_mask = y_train != -1
#     X_train_labeled = X_train[train_mask]
#     scaler = StandardScaler().fit(X_train_labeled)
#
#     model = MalwareMLP()
#     model.load_state_dict(torch.load("mlp_model.pth"))
#     model.eval()
#
#     # --- 3. Extract and Scale Features from Victim File ---
#     print(f"Processing victim file: '{victim_file_path}'")
#     with open(victim_file_path, "rb") as f:
#         file_bytes = f.read()
#
#     extractor = ember.PEFeatureExtractor(feature_version=2)
#     raw_features = extractor.raw_features(file_bytes)
#     original_vector = extractor.process_raw_features(raw_features)
#
#     # Scale the features
#     scaled_vector = scaler.transform(original_vector.reshape(1, -1))
#
#     # Convert to a PyTorch tensor that requires a gradient
#     input_tensor = torch.tensor(scaled_vector, dtype=torch.float32, requires_grad=True)
#
#     # --- 4. Confirm Initial "Malicious" Classification ---
#     with torch.no_grad():
#         initial_score = model(input_tensor).item()
#     print(f"\nInitial classification score: {initial_score:.4f} -> {'MALICIOUS' if initial_score > 0.5 else 'BENIGN'}")
#     if initial_score <= 0.5:
#         print("Warning: The initial file is already classified as benign. The attack may not be meaningful.")
#
#     # --- 5. Perform the FGSM Attack ---
#     print(f"Performing FGSM attack with epsilon = {epsilon}...")
#
#     # Define our target: we want the model to think this is BENIGN (label 0)
#     target_label = torch.tensor([[0.0]], dtype=torch.float32)
#     criterion = nn.BCELoss()
#
#     # Calculate the loss
#     output = model(input_tensor)
#     loss = criterion(output, target_label)
#
#     # Zero out any existing gradients, calculate the new gradient
#     model.zero_grad()
#     loss.backward()
#
#     # Collect the gradient of the loss with respect to the input features
#     gradient = input_tensor.grad.data
#
#     # Get the sign of the gradient
#     sign_of_gradient = torch.sign(gradient)
#
#     # Create the adversarial sample by adjusting the original input
#     # in the opposite direction of the gradient (gradient descent)
#     adversarial_tensor = input_tensor - epsilon * sign_of_gradient
#
#     # --- 6. Confirm Evasion Success ---
#     with torch.no_grad():
#         final_score = model(adversarial_tensor).item()
#     print(f"Adversarial classification score: {final_score:.4f} -> {'MALICIOUS' if final_score > 0.5 else 'BENIGN'}")
#
#     if final_score <= 0.5:
#         print("\nAttack successful! The model was fooled.")
#     else:
#         print("\nAttack failed. Try increasing epsilon or a different attack method.")
#
#     # --- 7. Save Vectors for Analysis in Act 3 ---
#     print("Saving original and adversarial (scaled) feature vectors for analysis...")
#     np.save("original_scaled_vector.npy", input_tensor.detach().numpy())
#     np.save("adversarial_scaled_vector.npy", adversarial_tensor.detach().numpy())
#     print("Files 'original_scaled_vector.npy' and 'adversarial_scaled_vector.npy' saved.")
#
#
# if __name__ == '__main__':
#     victim_file = "toy_malware.exe"
#     # Epsilon is the "strength" of the attack. It's a small number that
#     # determines how much we change the features. Let's start with a small value.
#     epsilon_value = 0.1
#
#     if not os.path.exists(victim_file):
#         print(f"Error: Victim file '{victim_file}' not found. Please ensure it's in the correct directory.")
#     else:
#         perform_attack(victim_file, epsilon_value)
