{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd00cd8",
   "metadata": {},
   "source": [
    "# Welcome to Adversarial ML with Images! üëã\n",
    "\n",
    "In this section, you‚Äôll see how small, nearly invisible pixel changes can trick a powerful image classifier. We'll use a **pre-trained ResNet-50 model** and implement two classic gradient-based attacks‚Äîthe **Fast Gradient Sign Method (FGSM)** and the more powerful **Projected Gradient Descent (PGD)**.\n",
    "\n",
    "The goal is to generate a perturbed image that looks unchanged to a human but causes the model to make a completely different prediction. As a final step, you'll learn to embed your adversarial creation back into the original high-resolution image.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Do Here üéØ\n",
    "\n",
    "1.  **Load the model** and classify a clean image to see its baseline prediction.\n",
    "2.  **Implement FGSM** by writing the code for a single, powerful attack step.\n",
    "3.  **Extend FGSM into PGD**, turning the single step into an iterative attack.\n",
    "4.  **Generate a targeted example** that forces the model to predict a specific class.\n",
    "5.  **Try it on your own image** to see how different subjects react.\n",
    "6.  **(Optional)** If you finish early, try the challenges at the end.\n",
    "\n",
    "---\n",
    "> ‚ö†Ô∏è **Educational Use Only:** These techniques are for research and learning. Please use this knowledge responsibly to build more robust systems, not to deceive real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e514b2",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the Environment\n",
    "\n",
    "The first code cell below is a one-time setup step. It performs three key actions:\n",
    "1.  **Clones the GitHub repository:** This downloads all our workshop files, including helper code and the sample images.\n",
    "2.  **Changes the directory:** It navigates into the newly downloaded folder so we can access those files.\n",
    "3.  **Installs packages:** It installs the specific libraries listed in `requirements.txt`.\n",
    "\n",
    "‚ñ∂Ô∏è **Please run this cell now.** It might take a minute to complete. You'll see a \"‚úÖ Setup complete!\" message when it's finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "\n",
    "repo_url = \"https://github.com/japheth45/Adversarial_ML25.git\"\n",
    "repo_dir = \"Adversarial_ML25/image_classification\"\n",
    "\n",
    "# Clone the repository\n",
    "!git clone {repo_url}\n",
    "\n",
    "# Check if the clone was successful by seeing if the directory exists\n",
    "if os.path.isdir(repo_dir):\n",
    "  print(\"‚úÖ Repository cloned successfully.\")\n",
    "  %cd {repo_dir}\n",
    "  print(\"Installing required packages...\")\n",
    "  !pip install -q -r requirements.txt # The -q flag makes the output quieter\n",
    "  print(\"‚úÖ Setup complete!\")\n",
    "else:\n",
    "  print(\"‚ùå Error: Could not clone the repository.\")\n",
    "  print(\"Please check the URL and ensure the repository is public.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc1473",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Now that our environment is ready, this cell imports all the necessary tools for our session. This includes standard libraries like `torch` and `PIL` (for image handling), as well as the custom helper functions we've written for this workshop (like `classify` and `load_image`).\n",
    "\n",
    "‚ñ∂Ô∏è **Run this cell to load everything into memory.** It should complete in under a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f365a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports & device ---\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import image_helpers\n",
    "import torch, torchvision\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13039b56",
   "metadata": {},
   "source": [
    "## Step 3: Test Drive: classify the sample image\n",
    "\n",
    "Before we try to fool the network, let‚Äôs see how it behaves on an ordinary photo.\n",
    "This cell loads `data/dog.jpg`, displays it, and asks our pre-trained ResNet-50 to predict the **Top-5** ImageNet classes with confidences. We then capture the model‚Äôs top prediction in `original_label` so later attack cells can compare against (and, ideally, flip) this baseline.\n",
    "\n",
    "- `classify(...)` handles preprocessing (resize, center-crop, normalization), runs the model, and returns a list of `(label, confidence)` pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940b861",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "k = 5  # Set the number of top predictions to show\n",
    "img_path = \"data/dog.jpg\"\n",
    "\n",
    "# Show the original image\n",
    "print(\"Original Image:\")\n",
    "display(Image.open(img_path))\n",
    "\n",
    "# Get and display the top k predictions\n",
    "predictions = image_helpers.classify(img_path, k=k)\n",
    "image_helpers.print_topk(predictions, k=k)\n",
    "\n",
    "# Get the single top prediction (the most likely class)\n",
    "original_label = predictions[0][0]\n",
    "print(f\"\\nConclusion: The model is confident this is a '{original_label}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f7b5b",
   "metadata": {},
   "source": [
    "## 4) Exercise A ‚Äî Single FGSM step (targeted)\n",
    "\n",
    "Implement a **single step** that pushes the model **toward a target class** by *decreasing* cross‚Äëentropy wrt that target.\n",
    "Fill in the two TODOs below:\n",
    "- Update `delta` using the sign of the gradient (constrain to `[-eps, eps]`).\n",
    "- Reset gradients to avoid accumulation.\n",
    "\n",
    "Completing this code just builds our FGSM function - we'll use this function in a cell below...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f22b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_step(x, y_target, model, delta, alpha, eps):\n",
    "    \"\"\"One targeted FGSM step on normalized input space.\"\"\"\n",
    "    delta.requires_grad_(True)\n",
    "    logits = model(x + delta)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y_target)  # targeted: minimize loss to increase target prob\n",
    "    loss.backward()\n",
    "\n",
    "# TODO: Construct the update rule for delta. This requires four steps (but just one line of code):\n",
    "    #   1. Get the direction of the gradient using .grad.sign()\n",
    "    #   2. Scale that direction by the step size, `alpha`\n",
    "    #   3. Subtract the result from the current `delta` to perform gradient descent\n",
    "    #   4. Clamp the final value to keep it within our budget of [-eps, eps]\n",
    "\n",
    "    # delta.data = ...\n",
    "\n",
    "\n",
    "# TODO: zero the gradients on delta to prevent accumulation across steps\n",
    "    # delta...\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        target_prob = float(probs[0, y_target.item()].item())\n",
    "    return delta.detach(), float(loss.item()), target_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caa802",
   "metadata": {},
   "source": [
    "### (Optional) Solution for Exercise A\n",
    "\n",
    "Expand and run if you‚Äôre stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06958044",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@title üîë Solution: Targeted FGSM Step (Click to Expand)\n",
    "def fgsm_step(x, y_target, model, delta, alpha, eps):\n",
    "    delta.requires_grad_(True)\n",
    "    logits = model(x + delta)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y_target)\n",
    "    loss.backward()\n",
    "    # targeted: gradient descent on loss\n",
    "    delta.data = (delta - alpha * delta.grad.sign()).clamp(-eps, eps)\n",
    "    delta.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        target_prob = float(probs[0, y_target.item()].item())\n",
    "    return delta.detach(), float(loss.item()), target_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493efd28",
   "metadata": {},
   "source": [
    "## 5) Exercise B ‚Äî PGD (loop + early stopping)\n",
    "\n",
    "Turn your FGSM step into **PGD** by looping it. Stop early once the target confidence exceeds a threshold.  \n",
    "Fill in the TODOs below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ef8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise B: PGD loop with early stopping ---\n",
    "def pgd_targeted(image_path, target_label_name, eps=8/255, alpha=2/255, iters=50, stop_threshold=0.9):\n",
    "    \"\"\"Targeted PGD on a single 224x224 crop (normalized space); returns an *unnormalized* tensor in [0,1].\"\"\"\n",
    "    model, dev = image_helpers.get_model()\n",
    "    x, _ = image_helpers.load_image(image_path)  # x is normalized to ImageNet mean/std, shape (1,3,224,224)\n",
    "    target_idx = image_helpers.label_to_index(target_label_name)\n",
    "    y_target = torch.tensor([target_idx], device=dev)\n",
    "\n",
    "    # mean/std for un-normalizing later\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=dev).view(3,1,1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=dev).view(3,1,1)\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "\n",
    "    for i in range(iters):\n",
    "        # TODO 1: perform a single targeted FGSM step by calling the `fgsm_step` function.\n",
    "        # (Review the FGSM function in the previous cell for needed arguments.)\n",
    "        # Make sure to capture the three return values into the specified variables.\n",
    "\n",
    "        # delta, loss_val, target_prob = fgsm_step(...)\n",
    "\n",
    "        # TODO 2: Update the print statement below to print the variables you received\n",
    "        # from the `fgsm_step` function.  (Replace ??? with your variables)\n",
    "\n",
    "        # print(f\"Iteration {i+1:02d}: loss=???  target_conf=???\")\n",
    "\n",
    "        # TODO 3: Write an `if` statement to check if the `target_prob`\n",
    "        # is greater than the `stop_threshold`. If it is, print a success message\n",
    "        # and use the `break` command to exit the loop.\n",
    "\n",
    "        # if ...:\n",
    "        #     print(f\"‚úÖ Early stop at iter {i+1}\")\n",
    "        #     break\n",
    "\n",
    "    # Compose final adversarial and un-normalize back to pixel space\n",
    "    adv_x = x + delta\n",
    "    adv_unnorm = (adv_x * std + mean).clamp(0, 1)\n",
    "    return adv_unnorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e026bd3",
   "metadata": {},
   "source": [
    "### (Optional) Solution for Exercise B\n",
    "\n",
    "Expand and run if you‚Äôre stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc11c42",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@title üîë Solution: PGD_Targeted function (Click to Expand)\n",
    "def pgd_targeted(image_path, target_label_name, eps=8/255, alpha=2/255, iters=50, stop_threshold=0.9):\n",
    "    model, dev = image_helpers.get_model()\n",
    "    x, _ = image_helpers.load_image(image_path)\n",
    "    target_idx = image_helpers.label_to_index(target_label_name)\n",
    "    y_target = torch.tensor([target_idx], device=dev)\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=dev).view(3,1,1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=dev).view(3,1,1)\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "\n",
    "    for i in range(iters):\n",
    "        delta, loss_val, target_prob = fgsm_step(x, y_target, model, delta, alpha, eps)\n",
    "        print(f\"Iteration {i+1:02d}: loss={loss_val:.4f}  target_conf={target_prob:.2%}\")\n",
    "        if target_prob > stop_threshold:\n",
    "            print(f\"‚úÖ Early stop at iter {i+1} (confidence {target_prob:.2%})\")\n",
    "            break\n",
    "\n",
    "    adv_x = x + delta\n",
    "    adv_unnorm = (adv_x * std + mean).clamp(0, 1)\n",
    "    return adv_unnorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3104c2c",
   "metadata": {},
   "source": [
    "## 6) Run your targeted attack\n",
    "\n",
    "Pick a target label (string match against ImageNet label names). The helper `label_to_index(...)` resolves it to an index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a044b40",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Run targeted PGD on the sample image ---\n",
    "target_label = \"chain saw\"   # change this to try other targets\n",
    "adv_tensor = pgd_targeted(img_path, target_label, eps=8/255, alpha=2/255, iters=100, stop_threshold=0.9)\n",
    "\n",
    "# Save and display the 224x224 adversarial crop\n",
    "adv_patch_path = \"data/dog_adv_patch.png\"\n",
    "image_helpers.save_tensor_image(adv_tensor, adv_patch_path)\n",
    "print(f\"Saved adversarial crop to: {adv_patch_path}\")\n",
    "display(Image.open(adv_patch_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf7912",
   "metadata": {},
   "source": [
    "## 7) Paste the adversarial crop back into the full image\n",
    "\n",
    "We‚Äôll re‚Äëembed the 224√ó224 adversarial crop into the original‚Äëresolution image and classify both for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d511a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Embed crop back and compare ---\n",
    "full_adv_path = \"data/dog_adv_full.jpg\"\n",
    "image_helpers.embed_crop_back(img_path, Image.open(adv_patch_path), save_path=full_adv_path)\n",
    "\n",
    "print(\"\\n--- Original image ---\")\n",
    "image_helpers.print_topk(image_helpers.classify(img_path))\n",
    "\n",
    "print(\"\\n--- Full-size adversarial image ---\")\n",
    "image_helpers.print_topk(image_helpers.classify(full_adv_path))\n",
    "\n",
    "print(\"\\nVisual check (original vs adversarial):\")\n",
    "display(Image.open(img_path))\n",
    "display(Image.open(full_adv_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15c6db",
   "metadata": {},
   "source": [
    "## 8) Try your own image (optional)\n",
    "\n",
    "Find an image on the web, paste the URL below, and see what the model thinks it is. Right-\n",
    "click on an image online and select \"Copy Image Address\" or a similar option to get the URL.\n",
    "\n",
    "(Or upload a local file and set `local_path`) and pick a target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770dc0ef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Your own image attack (optional) ---\n",
    "import os, urllib.request\n",
    "\n",
    "image_url = \"\"  # e.g., \"https://example.com/cat.jpg\"\n",
    "\n",
    "# \"https://images.stockcake.com/public/9/6/5/965857ff-5ab5-427f-859c-4487bffaf078_medium/shiny-truck-display-stockcake.jpg\"\n",
    "\n",
    "local_path = \"\" # e.g., \"my_photo.jpg\" (if you uploaded a file)\n",
    "\n",
    "if image_url and not local_path:\n",
    "    local_path = \"data/my_image\" + os.path.splitext(image_url)[1]\n",
    "    try:\n",
    "        urllib.request.urlretrieve(image_url, local_path)\n",
    "        print(f\"Downloaded to {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Download failed:\", e)\n",
    "\n",
    "if local_path and os.path.exists(local_path):\n",
    "    new_target = \"tennis ball\"  # change to experiment\n",
    "    adv_tensor2 = pgd_targeted(local_path, new_target, eps=8/255, alpha=2/255, iters=100, stop_threshold=0.90)\n",
    "\n",
    "    root, ext = os.path.splitext(local_path)\n",
    "    adv_patch2 = f\"{root}_adv_patch.png\"\n",
    "    image_helpers.save_tensor_image(adv_tensor2, adv_patch2)\n",
    "\n",
    "    # Determine output format (PNG for .png inputs, otherwise JPEG)\n",
    "    out_ext = \"PNG\" if ext.lower() == \".png\" else \"JPEG\"\n",
    "    adv_full2 = f\"{root}_adv_full{ext if ext else '.jpg'}\"\n",
    "    image_helpers.embed_crop_back(local_path, Image.open(adv_patch2), save_path=adv_full2, fmt=out_ext)\n",
    "\n",
    "    # --- Original image and classification ---\n",
    "    print(\"\\n--- Original Image Predictions ---\")\n",
    "    image_helpers.print_topk(image_helpers.classify(local_path))\n",
    "    print(\"Original Image:\", local_path)\n",
    "    display(Image.open(local_path))\n",
    "\n",
    "    # --- Altered image and classification ---\n",
    "    print(\"\\n--- Adversarial Image Predictions ---\")\n",
    "    image_helpers.print_topk(image_helpers.classify(adv_full2))\n",
    "    print(\"\\nAdversarial Image:\", adv_full2)\n",
    "    display(Image.open(adv_full2))\n",
    "else:\n",
    "    print(\"Set image_url or local_path to try your own image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b2aab",
   "metadata": {},
   "source": [
    "## 9) Optional Challenges ‚Äî Explore, extend, and have fun\n",
    "\n",
    "If you finish early (or later at home), try these small challenges to deepen your intuition about adversarial ML on images. Each one builds on what we did in the main notebook. Skeleton code is provided so you can focus on the ideas. Feel free to use online resources (including generative AI) to help you implement the pieces.\n",
    "\n",
    "**What to expect**\n",
    "- You will reuse helpers from `image_helpers.py` and patterns from the earlier FGSM/PGD cells.\n",
    "- Success criteria are concrete (for example, a margin over the original class, or a target confidence threshold).\n",
    "- Keep images in [0,1] when saving; do model math in normalized space.\n",
    "\n",
    "**Challenges**\n",
    "1. **Untargeted attack**\n",
    "   Push *away* from the original top class by maximizing its loss (negative log probability). Declare success when another class is higher by at least a chosen margin (for example, 0.05).\n",
    "\n",
    "2. **Saliency + Grad-CAM**\n",
    "   Visualize where the model ‚Äúlooks.‚Äù Compute a vanilla saliency map from input gradients and a Grad-CAM heatmap from a late conv layer. Overlay on the image to compare attention patterns.\n",
    "\n",
    "3. **Robustness check (re-encode and re-scale)**\n",
    "   Take a 224√ó224 adversarial crop, embed it back into the original at several output sizes and JPEG qualities, save, reload, and re-classify. Watch how confidence changes as resizing and compression perturb the signal.\n",
    "\n",
    "4. **Image synthesis from gray/noise**\n",
    "   Start from a gray or random canvas and do gradient ascent on a target class score. Add light regularization (TV/L2) if you like. The result should be classifiable, even if it looks like class-specific texture rather than a photograph.\n",
    "\n",
    "Enjoy experimenting!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenge 1 ‚Äî Untargeted attack (reduce the true-class probability)\n",
    "\n",
    "Goal: make the model stop believing its original top-1 prediction (`y_true`).\n",
    "Instead of pushing toward a specific target, push *away* from `y_true` by maximizing the loss for that class (negative log probability of `y_true`). Declare success when some other class is higher than `y_true` by at least a small margin (for example, `margin = 0.05`) to avoid flicker between classes.\n",
    "\n",
    "What to build:\n",
    "- An **untargeted FGSM step** that increases the true-class loss and keeps the perturbation within an L_inf bound.\n",
    "- An **untargeted PGD loop** that repeats FGSM-style steps, projects back to the L_inf ball around the original, clamps to `[0,1]`, and early-stops when the margin condition is met.\n",
    "\n",
    "Use the same normalization space as the targeted code. You already have helpers in `image_helpers.py` and a targeted PGD example; mirror that pattern but flip the objective away from `y_true`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Challenge 1 (Skeleton): Untargeted FGSM/PGD using image_helpers -------\n",
    "# Keep this minimal. Fill in the bodies as you see fit.\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import image_helpers  # get_model, load_image, label_to_index, classify_tensor, save_tensor_image\n",
    "\n",
    "def fgsm_step_untargeted(\n",
    "    x_norm: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    model,\n",
    "    delta: torch.Tensor,\n",
    "    alpha: float,\n",
    "    eps: float,\n",
    ") -> Tuple[torch.Tensor, float, float, float]:\n",
    "    \"\"\"\n",
    "    One untargeted FGSM step that increases the loss of the true class.\n",
    "    This function should:\n",
    "      - forward (x_norm + delta) through the model,\n",
    "      - compute negative log prob for y_true,\n",
    "      - take a gradient *ascent* step on delta,\n",
    "      - clamp delta to [-eps, +eps],\n",
    "      - return (delta, loss_value, p_true, p_top_other).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def pgd_untargeted(\n",
    "    image_path: str,\n",
    "    true_label_name: str,\n",
    "    eps: float = 8/255,\n",
    "    alpha: float = 2/255,\n",
    "    iters: int = 50,\n",
    "    margin: float = 0.05,\n",
    "    early_stop: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Untargeted PGD loop.\n",
    "    This function should:\n",
    "      - load a NORMALIZED tensor via image_helpers.load_image(image_path),\n",
    "      - build y_true from true_label_name using image_helpers.label_to_index,\n",
    "      - initialize a delta tensor (normalized space),\n",
    "      - iteratively call fgsm_step_untargeted,\n",
    "      - implement an early-stop when p_top_other >= p_true + margin,\n",
    "      - de-normalize to [0,1] before returning the adversarial image.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def success_margin(p_true: float, p_top_other: float, margin: float) -> bool:\n",
    "    \"\"\"\n",
    "    Return True when p_top_other >= p_true + margin.\n",
    "    You can call this from inside pgd_untargeted for early stopping.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Sample usage (fill bodies above, then run) -----------------------------\n",
    "# model, dev = image_helpers.get_model()\n",
    "# adv = pgd_untargeted(\n",
    "#     image_path=img_path,\n",
    "#     true_label_name=original_label,\n",
    "#     eps=8/255, alpha=2/255, iters=30, margin=0.05, early_stop=True\n",
    "# )\n",
    "# image_helpers.save_tensor_image(adv, \"adv_untargeted.png\")\n",
    "# print(image_helpers.classify_tensor(adv)[:5])\n",
    "\n",
    "# Tip: You can model this after your existing targeted utilities:\n",
    "# - fgsm_step(...) and pgd_targeted(...) are good references; flip the objective away from y_true.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenge 2 ‚Äî Saliency maps (vanilla gradients) and Grad-CAM\n",
    "\n",
    "Goal: visualize *where* the network is looking when it makes a prediction.\n",
    "\n",
    "Two complementary views:\n",
    "- **Vanilla saliency**: take the gradient of the score for a chosen class with respect to the input pixels. The absolute value (or max over channels) highlights pixels the score is most sensitive to.\n",
    "- **Grad-CAM**: use gradients flowing into a late convolutional block to weight its feature maps, then upsample to image size. This yields a coarse, interpretable heatmap over the object regions.\n",
    "\n",
    "What to build:\n",
    "- A function to compute a **vanilla saliency map** for a given image and class id.\n",
    "- A lightweight **Grad-CAM** helper that registers hooks on a chosen layer in ResNet-50 (for example, the last block in `layer4`), computes the class score, backprops, and returns an upsampled heatmap.\n",
    "- Optionally, a small utility to **overlay** the heatmap on the original image to make the visualization easier to read.\n",
    "\n",
    "Notes:\n",
    "- Work in the **normalized** space for the forward/backward pass, but produce heatmaps in image coordinates (224√ó224).\n",
    "- Try both the **top-1 class** and a **chosen class** to compare attention.\n",
    "- Extensions for early finishers: SmoothGrad (average saliency over noise), guided backprop, or combining Guided Backprop with Grad-CAM.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Challenge 2 (Skeleton): Saliency + Grad-CAM ---------------------------\n",
    "# Keep this minimal; fill the bodies as you like.\n",
    "\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import image_helpers  # get_model, load_image, LABELS\n",
    "\n",
    "def compute_vanilla_saliency(\n",
    "    x_norm: torch.Tensor,\n",
    "    class_idx: int,\n",
    "    model: torch.nn.Module,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a saliency map for the given class_idx at input resolution.\n",
    "    This function should:\n",
    "      - enable gradient on x_norm,\n",
    "      - forward to logits and select the score for class_idx,\n",
    "      - backprop to get d(score)/d(x_norm),\n",
    "      - convert to a single-channel saliency (e.g., abs max over channels),\n",
    "      - normalize to [0,1] and return a (1,1,H,W) or (1,H,W) tensor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Minimal Grad-CAM helper.\n",
    "    You should:\n",
    "      - register forward and backward hooks on a target conv module,\n",
    "      - cache feature maps (forward) and gradients (backward),\n",
    "      - compute channel weights from pooled gradients,\n",
    "      - make a weighted sum of feature maps,\n",
    "      - ReLU, normalize, and upsample to input size.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, target_module: torch.nn.Module):\n",
    "        # store refs, set placeholders for cached activations and grads, register hooks\n",
    "        pass\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove any registered hooks.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def compute(\n",
    "        self,\n",
    "        x_norm: torch.Tensor,\n",
    "        class_idx: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return a Grad-CAM heatmap (1,H,W) aligned to the input size.\n",
    "        This should:\n",
    "          - run a forward pass to get logits,\n",
    "          - backprop the score for class_idx,\n",
    "          - use cached feats/grads to build the heatmap,\n",
    "          - upsample to input resolution and normalize to [0,1].\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "def overlay_heatmap_on_pil(\n",
    "    heatmap: torch.Tensor,\n",
    "    image_pil: Image.Image,\n",
    "    alpha: float = 0.4,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Overlay a heatmap onto a PIL image and return a composite image.\n",
    "    This function should:\n",
    "      - convert the normalized heatmap to a colored mask,\n",
    "      - resize to match image_pil,\n",
    "      - alpha-blend heatmap and image.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Sample usage (after you fill in the bodies) ---------------------------\n",
    "# model, dev = image_helpers.get_model()\n",
    "# x_norm, pil_img = image_helpers.load_image(img_path)\n",
    "#\n",
    "# # 1) Vanilla saliency for top-1 class\n",
    "# with torch.no_grad():\n",
    "#     logits = model(x_norm)\n",
    "#     class_idx = int(logits.argmax(dim=1).item())\n",
    "# sal = compute_vanilla_saliency(x_norm.clone(), class_idx, model)\n",
    "#\n",
    "# # 2) Grad-CAM on the last conv block of ResNet-50\n",
    "# #    Example target module (verify in your model printout):\n",
    "# #    target_module = model.layer4[-1].conv3  or  model.layer4[-1]\n",
    "# #    (pick a conv module that produces feature maps)\n",
    "# # target_module = model.layer4[-1]\n",
    "# # cam = GradCAM(model, target_module)\n",
    "# # cam_map = cam.compute(x_norm.clone(), class_idx)\n",
    "# # cam.remove_hooks()\n",
    "#\n",
    "# # 3) Overlay and visualize\n",
    "# # composite = overlay_heatmap_on_pil(cam_map, pil_img, alpha=0.45)\n",
    "# # display(composite)\n",
    "#\n",
    "# # Try different classes:\n",
    "# # chosen_idx = image_helpers.LABELS.index(\"golden retriever\")  # or use label_to_index on a name\n",
    "# # sal2 = compute_vanilla_saliency(x_norm.clone(), chosen_idx, model)\n",
    "# # cam2 = GradCAM(model, model.layer4[-1]); cam_map2 = cam2.compute(x_norm.clone(), chosen_idx); cam2.remove_hooks()\n",
    "# # display(overlay_heatmap_on_pil(cam_map2, pil_img))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenge 3 ‚Äî Robustness check by re-encoding and re-scaling\n",
    "\n",
    "Goal: see how brittle a successful adversarial crop is once it‚Äôs saved back into the original photo at different sizes and JPEG qualities.\n",
    "\n",
    "You‚Äôll take a 224√ó224 adversarial crop (the same size we fed the model), **embed it back** into the full image at several output resolutions (for example, 400√ó600, 600√ó900, ‚Ä¶) and with different JPEG qualities (for example, 95, 85, 70). After each save, **reload and re-classify** to observe how confidence changes. Expect some attacks to weaken as resizing and compression dilute or shift the perturbation.\n",
    "\n",
    "What to build:\n",
    "- A small routine that loops over a list of output sizes and qualities, calls `embed_crop_back(...)`, saves each variant, and returns file paths.\n",
    "- A tester that reopens each saved image and records Top-1 / Top-5 predictions and confidence.\n",
    "- A tiny helper to convert your 224√ó224 adversarial tensor to a PIL image (if you don‚Äôt already have it as PIL).\n",
    "\n",
    "Tips:\n",
    "- Reuse earlier code: your adversarial result is already a 224√ó224 crop; convert it to PIL once and reuse.\n",
    "- Keep filenames informative (include size and quality) so results are easy to compare.\n",
    "- Try multiple images; some are more robust than others.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Challenge 3 (Skeleton): Embed adversarial crop at various sizes --------\n",
    "# Fill in the bodies as you like; reuse image_helpers wherever possible.\n",
    "\n",
    "from typing import Iterable, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import image_helpers  # embed_crop_back, classify, classify_tensor, save_tensor_image\n",
    "\n",
    "def to_pil_224(adv_tensor_unnorm: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a (1,3,224,224) adversarial tensor in [0,1] to a 224x224 PIL image.\n",
    "    This function should:\n",
    "      - detach/clip to [0,1],\n",
    "      - convert using torchvision or manual numpy path,\n",
    "      - return a PIL Image in RGB.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def generate_variants_with_embed(\n",
    "    original_path: str,\n",
    "    adv_crop_pil: Image.Image,\n",
    "    sizes: Iterable[Tuple[int,int]],\n",
    "    qualities: Iterable[int],\n",
    "    out_dir: str = \"robustness_variants\",\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Create resized/compressed variants by embedding adv_crop_pil into original_path.\n",
    "    This function should:\n",
    "      - ensure out_dir exists,\n",
    "      - for each (W,H) and quality:\n",
    "          - call image_helpers.embed_crop_back(original_path, adv_crop_pil, save_path, fmt=\"JPEG\", quality=quality)\n",
    "          - optionally resize to (W,H) *before saving* OR pass through embed_crop_back then resize,\n",
    "      - collect and return a list of saved file paths.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def evaluate_variants(paths: List[Path], topk: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-open each saved image and classify it.\n",
    "    This function should:\n",
    "      - loop over paths,\n",
    "      - call image_helpers.classify(str(path), k=topk),\n",
    "      - store results (path, top-1 label, top-1 prob, maybe full top-k).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def print_variant_report(results: List[Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print a small table from `evaluate_variants` results.\n",
    "    This function should:\n",
    "      - parse filename to show size/quality (if encoded in name),\n",
    "      - print top-1 label and confidence for quick comparison.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Sample usage (after you fill in the bodies) ----------------------------\n",
    "# original_path = img_path  # the clean, full-resolution source used earlier\n",
    "# # If your adversarial is a tensor crop (1,3,224,224) in [0,1]:\n",
    "# # adv_crop_pil = to_pil_224(adv_crop_tensor)\n",
    "# #\n",
    "# # If you already saved it as \"adv_crop_224.png\", you can simply:\n",
    "# # adv_crop_pil = Image.open(\"adv_crop_224.png\").convert(\"RGB\")\n",
    "#\n",
    "# sizes     = [(400, 600), (600, 900), (800, 1200)]   # (W, H)\n",
    "# qualities = [95, 85, 70]\n",
    "#\n",
    "# saved_paths = generate_variants_with_embed(\n",
    "#     original_path=original_path,\n",
    "#     adv_crop_pil=adv_crop_pil,\n",
    "#     sizes=sizes,\n",
    "#     qualities=qualities,\n",
    "#     out_dir=\"robustness_variants\"\n",
    "# )\n",
    "#\n",
    "# results = evaluate_variants(saved_paths, topk=5)\n",
    "# print_variant_report(results)\n",
    "#\n",
    "# # Optional: also test PNG (no JPEG artifacts):\n",
    "# # saved_paths_png = generate_variants_with_embed(original_path, adv_crop_pil, sizes=[(600,900)], qualities=[100])\n",
    "# # ... then evaluate as above.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenge 4 ‚Äî Image synthesis from gray/noise (feature visualization lite)\n",
    "\n",
    "Goal: start from a gray field or random noise and ‚Äúdream up‚Äù an input that the model classifies as a chosen target class. Instead of *reducing* confidence in the original label, we‚Äôll *increase* the score of a target class by taking gradient ascent steps on the input itself.\n",
    "\n",
    "What to build:\n",
    "- A canvas initializer that creates a 224√ó224 tensor in [0,1] (gray or noise).\n",
    "- A single ‚Äúascent step‚Äù that nudges pixels to increase the target class score. Optionally add light regularizers (for example, L2 toward gray and/or total variation) to keep the pattern from exploding.\n",
    "- A synth loop that repeats ascent steps, clamps to [0,1], and tracks the current confidence. Stop when you reach a confidence threshold or run out of iterations.\n",
    "\n",
    "Notes:\n",
    "- Work in the normalized space for the forward pass, but keep the master image in [0,1].\n",
    "- Small step sizes with many iterations tend to look cleaner; regularization weights can be tiny but helpful.\n",
    "- This produces classifiable textures, not photoreal images‚Äîgreat for discussing what the model ‚Äúkeys on.‚Äù\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Challenge 4 (Skeleton): Class \"dreaming\" via gradient ascent -----------\n",
    "# Minimal scaffolding; fill bodies as you like. Reuse image_helpers utilities.\n",
    "\n",
    "from typing import Literal, Tuple\n",
    "import torch\n",
    "import image_helpers  # get_model, label_to_index, classify_tensor, save_tensor_image\n",
    "\n",
    "CanvasMode = Literal[\"gray\", \"noise\"]\n",
    "\n",
    "def init_canvas(\n",
    "    mode: CanvasMode = \"gray\",\n",
    "    device: torch.device = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a (1,3,224,224) float tensor in [0,1] to serve as the starting image.\n",
    "    - mode=\"gray\": mid-gray canvas\n",
    "    - mode=\"noise\": uniform random in [0,1]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def normalize_for_model(x_unnorm: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert [0,1] tensor to normalized space expected by ResNet-50.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def total_variation(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a scalar TV penalty encouraging local smoothness.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def ascent_step(\n",
    "    x_unnorm: torch.Tensor,\n",
    "    y_target_idx: int,\n",
    "    model: torch.nn.Module,\n",
    "    alpha: float,\n",
    "    tv_weight: float = 0.0,\n",
    "    l2_weight: float = 0.0,\n",
    ") -> Tuple[torch.Tensor, float, float]:\n",
    "    \"\"\"\n",
    "    Take one gradient ascent step to increase the target class score.\n",
    "    This function should:\n",
    "      - enable grad on x_unnorm,\n",
    "      - forward normalized x to logits,\n",
    "      - form an objective: target log-prob minus small regularizers,\n",
    "      - backprop and nudge x_unnorm upward by alpha * sign(grad) or raw grad,\n",
    "      - clamp back to [0,1],\n",
    "      - return (updated_x, loss_value, target_confidence).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def synthesize_image(\n",
    "    target_label_name: str,\n",
    "    steps: int = 200,\n",
    "    alpha: float = 1.0/255,\n",
    "    tv_weight: float = 1e-4,\n",
    "    l2_weight: float = 1e-4,\n",
    "    start_mode: CanvasMode = \"gray\",\n",
    "    stop_threshold: float = 0.90,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Main loop: initialize a canvas, run gradient ascent, and return a [0,1] tensor.\n",
    "    This function should:\n",
    "      - get model/device, map label name -> index,\n",
    "      - init canvas via init_canvas(start_mode),\n",
    "      - iterate ascent_step(...) for `steps`,\n",
    "      - print/track current confidence and early-stop at stop_threshold,\n",
    "      - return the final [0,1] tensor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Sample usage (after you fill in the bodies) ----------------------------\n",
    "# model, dev = image_helpers.get_model()\n",
    "# adv_tex = synthesize_image(\n",
    "#     target_label_name=\"golden retriever\",\n",
    "#     steps=300,\n",
    "#     alpha=1.0/255,\n",
    "#     tv_weight=1e-4,\n",
    "#     l2_weight=1e-4,\n",
    "#     start_mode=\"noise\",\n",
    "#     stop_threshold=0.95\n",
    "# )\n",
    "# image_helpers.save_tensor_image(adv_tex, \"dream_golden.png\")\n",
    "# print(image_helpers.classify_tensor(adv_tex)[:5])\n",
    "\n",
    "# Variations:\n",
    "# - Try start_mode=\"gray\" for cleaner textures.\n",
    "# - Reduce alpha and increase steps for smoother patterns.\n",
    "# - Turn off TV/L2 to see the effect of regularization.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Workshop Auto-Generated"
   }
  ],
  "created": "2025-10-01T21:32:16.355044Z",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
